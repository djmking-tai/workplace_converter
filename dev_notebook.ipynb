{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def search_strings_in_json(directory, strings_to_find):\n",
    "    matches = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        content = f.read()\n",
    "                        for string in strings_to_find:\n",
    "                            if string in content.lower():\n",
    "                                matches.append((filepath, string))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {filepath}: {e}\")\n",
    "    return matches\n",
    "\n",
    "# strings_to_find = ['2307148732639320', '2090275401330376', '2090275404663709', '100056956201608']\n",
    "strings_to_find=['8073719999360913']\n",
    "\n",
    "#company info and company group:61565211886319 --> people_sets_1 & child_groups, 1910535509304367\n",
    "\n",
    "#7775445322516960\n",
    "#565159963841935\n",
    "#1910351919322726\n",
    "#100040716736805\n",
    "# directory = '/Users/dan/Documents/local_code/test/data/woolies/workplace/organization'\n",
    "# directory = '/Users/dan/Documents/local_code/test/data/woolies/workplace/groups'\n",
    "# directory = 'Workplace Data Company Information/groups'\n",
    "# directory = 'Workplace Data Company Information/organization'\n",
    "# directory = 'Workplace Data Company Information/user_profiles'\n",
    "directory = 'Workplace Data Company Information/groups'\n",
    "matches = search_strings_in_json(directory, strings_to_find)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for match in matches:\n",
    "    count+=1\n",
    "    print(f\"Found '{match[1]}' in file: {match[0]}\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON to pkl converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "def read_json_files(root_dir):\n",
    "    data = {}\n",
    "    if root_dir.endswith('.json'):\n",
    "        temp_key_name = root_dir.split('/')[-1].split('.')[0]\n",
    "        with open(root_dir, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "                data[temp_key_name] = json_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error reading {root_dir}: {e}\")\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.json'):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        temp_key_name = root_dir + '/' + file\n",
    "                        try:\n",
    "                            json_data = json.load(f)\n",
    "                            data[temp_key_name] = json_data\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error reading {filepath}: {e}\")\n",
    "    return data\n",
    "\n",
    "def extract_company_group(data_dict):\n",
    "    data_dict = list(data_dict.values())[0]\n",
    "    records = []\n",
    "    record_template = {(key, key): value for key, value in data_dict.items() if key != 'label_values'}\n",
    "    \n",
    "    \n",
    "\n",
    "    from copy import deepcopy\n",
    "\n",
    "    for item in data_dict['label_values']:\n",
    "        # print(item)\n",
    "        if not \"dict\" in item:\n",
    "            record_template[(item['ent_field_name'], item['label'])] = item.get('value', np.nan)\n",
    "\n",
    "    for item in data_dict['label_values']:\n",
    "        if \"dict\" not in item: continue\n",
    "\n",
    "        record = deepcopy(record_template)\n",
    "        # print(item)\n",
    "        record[(\"title\", \"title\")] = item['title']\n",
    "        record_copy = deepcopy(record)\n",
    "        \n",
    "        entry_list = item['dict']\n",
    "\n",
    "        for entry in entry_list:\n",
    "            for s_entry in entry['dict']:\n",
    "                # print(s_entry)\n",
    "\n",
    "                record_copy[(s_entry['ent_field_name'], s_entry.get('label', s_entry['ent_field_name']))] = s_entry['value']\n",
    "\n",
    "            records.append(deepcopy(record_copy))\n",
    "\n",
    "    column_index = pd.MultiIndex.from_tuples(list(records[0].keys()))\n",
    "    df = pd.DataFrame.from_records(records, columns=column_index)\n",
    "\n",
    "    # print(records)\n",
    "\n",
    "    return df\n",
    "\n",
    "# def extract_data(data, file_name = \"\"):\n",
    "#     # if file_name.startswith('company_group'):\n",
    "#     #     return extract_company_group(data)\n",
    "    \n",
    "#     records = []\n",
    "#     columns = set()\n",
    "\n",
    "#     def check_exists_append(record, key, value, columns):\n",
    "#         if record.get(key, False):\n",
    "#             if isinstance(record[key], list):\n",
    "#                 record[key].append(value)\n",
    "#             else:\n",
    "#                 record[key] = [record[key]] + [value]\n",
    "#         else:\n",
    "#             record[key] = value\n",
    "#             columns.add(key)\n",
    "\n",
    "#     def process_data(item, parent_record, parent_labels):\n",
    "#         new_record = 0\n",
    "#         # Copy the parent record to avoid mutation\n",
    "#         record = parent_record.copy()\n",
    "#         labels = parent_labels.copy()\n",
    "\n",
    "#         # Make a new entry if we're at the top level\n",
    "#         for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "#             if key in item:\n",
    "#                 new_record = 1\n",
    "\n",
    "#         if new_record == 1:\n",
    "#             record = {}.copy()\n",
    "\n",
    "#         # Extract basic fields\n",
    "#         for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "#             if key in item:\n",
    "#                 record[key] = item[key]\n",
    "#                 columns.add((key, \"\"))\n",
    "\n",
    "#         # Process 'label_values'\n",
    "#         label_values = item.get('label_values', [])\n",
    "#         if label_values:\n",
    "#             for lv in label_values:\n",
    "#                 ent_field_name = lv.get('ent_field_name', '')\n",
    "#                 label = lv.get('label', '')\n",
    "#                 key_name = [ent_field_name, label]\n",
    "\n",
    "#                 if 'value' in lv or 'timestamp_value' in lv:\n",
    "#                     value = lv.get('value') or lv.get('timestamp_value')\n",
    "#                     if labels:\n",
    "#                         check_exists_append(record, tuple(labels), value, columns)\n",
    "\n",
    "#                         # records.append(record.copy())\n",
    "#                     else:\n",
    "#                         check_exists_append(record, tuple(key_name), value, columns)\n",
    "#                 elif 'vec' in lv:\n",
    "#                     # Handle 'dict' and 'vec' recursively\n",
    "#                     nested_items = lv.get('vec')\n",
    "#                     if nested_items:\n",
    "#                         for nested_item in nested_items:\n",
    "#                             process_data({'label_values': [nested_item]}, parent_record, key_name )\n",
    "#                 elif 'dict' in lv:\n",
    "#                     # print(lv)\n",
    "#                     for entry in lv.get('dict', {}):\n",
    "#                         value = entry.get('value') or entry.get('timestamp_value')\n",
    "#                         temp_label = entry.get('label', '')\n",
    "#                         temp_key_name = (ent_field_name, temp_label)\n",
    "#                         if labels:\n",
    "#                             check_exists_append(record, tuple(labels + [temp_label]), value, columns)\n",
    "#                         else:\n",
    "#                             check_exists_append(record, tuple(key_name + [temp_label]), value, columns)\n",
    "#         else:\n",
    "#             # Handle case for generic JSON file\n",
    "#             for key, value in item.items():\n",
    "#                 check_exists_append(record, tuple([key]), value, columns)\n",
    "#         records.append(record.copy())\n",
    "\n",
    "#     # Iterate through a list\n",
    "#     for data_value in data.values():\n",
    "#         if isinstance(data_value, list):\n",
    "#             for item in data_value:\n",
    "#                 process_data(item, {}, [])\n",
    "#         else:\n",
    "#             process_data(data_value, {}, [])\n",
    "\n",
    "\n",
    "#     def create_multiindex_df(data):\n",
    "#         # Function to flatten dictionary and preserve both tuple elements\n",
    "#         def flatten_dict(d):\n",
    "#             flattened = {}\n",
    "#             for k, v in d.items():\n",
    "#                 if isinstance(k, tuple):\n",
    "#                     # Store both parts of the tuple\n",
    "#                     flattened[k] = v\n",
    "#                 else:\n",
    "#                     # For non-tuple keys, create a tuple with same value\n",
    "#                     flattened[(k, k)] = v\n",
    "#             return flattened\n",
    "        \n",
    "#         # Flatten all dictionaries\n",
    "#         flattened_data = [flatten_dict(d) for d in data]\n",
    "        \n",
    "#         # Create initial DataFrame\n",
    "#         df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "#         # Get all unique column tuples\n",
    "#         columns = df.columns.tolist()\n",
    "        \n",
    "\n",
    "#         # Create MultiIndex columns\n",
    "#         multi_index = pd.MultiIndex.from_tuples(columns)\n",
    "        \n",
    "#         # Create final DataFrame with MultiIndex\n",
    "#         final_df = pd.DataFrame(df.values, columns=multi_index)\n",
    "        \n",
    "#         return final_df\n",
    "#     return create_multiindex_df(records)\n",
    "\n",
    "def extract_data_old(data, file_name = \"\"):\n",
    "    records = []\n",
    "    columns = set()\n",
    "\n",
    "    def check_exists_append(record, key, value, columns):\n",
    "        if record.get(key, False):\n",
    "            if isinstance(record[key], list):\n",
    "                record[key].append(value)\n",
    "            else:\n",
    "                record[key] = [record[key]] + [value]\n",
    "        else:\n",
    "            record[key] = value\n",
    "            columns.add(key)\n",
    "\n",
    "    def process_vec_items(vec_items, record, key_name, parent_labels):\n",
    "        \"\"\"Helper function to process vector items recursively\"\"\"\n",
    "        if vec_items:\n",
    "            for nested_item in vec_items:\n",
    "                process_data({'label_values': [nested_item]}, record, key_name)\n",
    "\n",
    "    def process_dict_items(dict_items, record, ent_field_name, labels):\n",
    "        \"\"\"Helper function to process dictionary items recursively\"\"\"\n",
    "        for entry in dict_items:\n",
    "            value = entry.get('value') or entry.get('timestamp_value')\n",
    "            temp_label = entry.get('label', '')\n",
    "            \n",
    "            # Handle nested vectors within dictionary\n",
    "            if 'vec' in entry:\n",
    "                nested_key_name = labels + [temp_label] if temp_label else labels + [ent_field_name]\n",
    "                process_vec_items(entry['vec'], record, nested_key_name, labels)\n",
    "            # Handle nested dictionaries\n",
    "            elif 'dict' in entry:\n",
    "                nested_labels = labels + [temp_label] if temp_label else labels + [ent_field_name]\n",
    "                process_dict_items(entry['dict'], record, ent_field_name, nested_labels)\n",
    "            # Handle leaf values\n",
    "            else:\n",
    "                if labels:\n",
    "                    if temp_label:\n",
    "                        check_exists_append(record, tuple(labels + [temp_label]), value, columns)\n",
    "                    else:\n",
    "                        check_exists_append(record, tuple(labels + [ent_field_name]), value, columns)\n",
    "                else:\n",
    "                    check_exists_append(record, tuple([ent_field_name, temp_label]), value, columns)\n",
    "\n",
    "    def process_data(item, parent_record, parent_labels):\n",
    "        new_record = 0\n",
    "        record = parent_record.copy()\n",
    "        labels = parent_labels.copy()\n",
    "\n",
    "        # Make a new entry if we're at the top level\n",
    "        for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "            if key in item:\n",
    "                new_record = 1\n",
    "\n",
    "        if new_record == 1:\n",
    "            record = {}.copy()\n",
    "\n",
    "        # Extract basic fields\n",
    "        for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "            if key in item:\n",
    "                record[key] = item[key]\n",
    "                columns.add((key, \"\"))\n",
    "\n",
    "        # Process 'label_values'\n",
    "        label_values = item.get('label_values', [])\n",
    "        if label_values:\n",
    "            for lv in label_values:\n",
    "                ent_field_name = lv.get('ent_field_name', '')\n",
    "                label = lv.get('label', '')\n",
    "                key_name = [ent_field_name, label]\n",
    "\n",
    "                if 'value' in lv or 'timestamp_value' in lv:\n",
    "                    value = lv.get('value') or lv.get('timestamp_value')\n",
    "                    if labels:\n",
    "                        check_exists_append(record, tuple(labels), value, columns)\n",
    "                    else:\n",
    "                        check_exists_append(record, tuple(key_name), value, columns)\n",
    "                elif 'vec' in lv:\n",
    "                    # Handle vector items recursively\n",
    "                    process_vec_items(lv['vec'], record, key_name, labels)\n",
    "                elif 'dict' in lv:\n",
    "                    # Handle dictionary items recursively\n",
    "                    process_dict_items(lv['dict'], record, ent_field_name, \n",
    "                                    labels if labels else key_name)\n",
    "\n",
    "        else:\n",
    "            # Handle case for generic JSON file\n",
    "            for key, value in item.items():\n",
    "                check_exists_append(record, tuple([key]), value, columns)\n",
    "\n",
    "        records.append(record.copy())\n",
    "\n",
    "    # Iterate through a list\n",
    "    for data_value in data.values():\n",
    "        if isinstance(data_value, list):\n",
    "            for item in data_value:\n",
    "                process_data(item, {}, [])\n",
    "        else:\n",
    "            process_data(data_value, {}, [])\n",
    "\n",
    "    def create_multiindex_df(data):\n",
    "        # Function to flatten dictionary and preserve both tuple elements\n",
    "        def flatten_dict(d):\n",
    "            flattened = {}\n",
    "            for k, v in d.items():\n",
    "                if isinstance(k, tuple):\n",
    "                    # Store both parts of the tuple\n",
    "                    flattened[k] = v\n",
    "                else:\n",
    "                    # For non-tuple keys, create a tuple with same value\n",
    "                    flattened[(k, k)] = v\n",
    "            return flattened\n",
    "        \n",
    "        # Flatten all dictionaries\n",
    "        flattened_data = [flatten_dict(d) for d in data]\n",
    "        \n",
    "        # Create initial DataFrame\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "        # Get all unique column tuples\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        # Create MultiIndex columns\n",
    "        multi_index = pd.MultiIndex.from_tuples(columns)\n",
    "        \n",
    "        # Create final DataFrame with MultiIndex\n",
    "        final_df = pd.DataFrame(df.values, columns=multi_index)\n",
    "\n",
    "        def remove_duplicates_with_lists(df):\n",
    "            # Convert lists to strings for comparison\n",
    "            df_comparable = df.copy()\n",
    "            \n",
    "            # Convert empty lists to a string representation\n",
    "            df_comparable = df_comparable.applymap(lambda x: str(x) if isinstance(x, list) else x)\n",
    "            \n",
    "            # Drop duplicates based on all columns except the first one (assuming first is index)\n",
    "            # Keep first occurrence and preserve order\n",
    "            unique_indices = df_comparable.iloc[:, 1:].drop_duplicates().index\n",
    "            \n",
    "            # Return original dataframe with duplicates removed\n",
    "            return df.iloc[unique_indices]\n",
    "        \n",
    "        final_df_no_duplicates = remove_duplicates_with_lists(final_df)\n",
    "        \n",
    "        return final_df_no_duplicates\n",
    "    \n",
    "    return create_multiindex_df(records)\n",
    "\n",
    "# def extract_data(data, file_name = \"\"):\n",
    "#     records = []\n",
    "#     columns = set()\n",
    "\n",
    "#     def check_exists_append(record, key, value, columns, force_new=False):\n",
    "#         \"\"\"\n",
    "#         Append function that combines values into lists unless force_new is True\n",
    "#         \"\"\"\n",
    "#         if force_new:\n",
    "#             record[key] = value\n",
    "#             columns.add(key)\n",
    "#         else:\n",
    "#             if record.get(key, False):\n",
    "#                 if isinstance(record[key], list):\n",
    "#                     record[key].append(value)\n",
    "#                 else:\n",
    "#                     record[key] = [record[key]] + [value]\n",
    "#             else:\n",
    "#                 record[key] = value\n",
    "#                 columns.add(key)\n",
    "\n",
    "#     def process_vec_items(vec_items, base_record, key_name, parent_labels):\n",
    "#         \"\"\"Helper function to process vector items recursively\"\"\"\n",
    "#         if vec_items:\n",
    "#             for nested_item in vec_items:\n",
    "#                 process_data({'label_values': [nested_item]}, base_record.copy(), key_name)\n",
    "\n",
    "#     def has_nested_dict(dict_items):\n",
    "#         \"\"\"Check if there are nested dictionaries in the dict_items\"\"\"\n",
    "#         for entry in dict_items:\n",
    "#             if 'dict' in entry:\n",
    "#                 return True\n",
    "#         return False\n",
    "\n",
    "#     def process_dict_items(dict_items, base_record, ent_field_name, labels):\n",
    "#         \"\"\"Helper function to process dictionary items recursively\"\"\"\n",
    "#         has_nested = has_nested_dict(dict_items)\n",
    "#         current_record = base_record.copy()\n",
    "        \n",
    "#         for entry in dict_items:\n",
    "#             value = entry.get('value') or entry.get('timestamp_value')\n",
    "#             temp_label = entry.get('label', '')\n",
    "#             nested_ent_field_name = entry.get('ent_field_name', '')\n",
    "            \n",
    "#             # If there's an ent_field_name in the entry, use it\n",
    "#             current_field_name = nested_ent_field_name if nested_ent_field_name else ent_field_name\n",
    "            \n",
    "#             # Handle nested vectors within dictionary\n",
    "#             if 'vec' in entry:\n",
    "#                 nested_key_name = labels + [temp_label or current_field_name]\n",
    "#                 process_vec_items(entry['vec'], current_record, nested_key_name, labels)\n",
    "#             # Handle nested dictionaries\n",
    "#             elif 'dict' in entry:\n",
    "#                 nested_labels = labels + [temp_label or current_field_name]\n",
    "#                 process_dict_items(entry['dict'], current_record, current_field_name, nested_labels)\n",
    "#             # Handle leaf values\n",
    "#             else:\n",
    "#                 if labels:\n",
    "#                     key = tuple(labels + [temp_label or current_field_name])\n",
    "#                 else:\n",
    "#                     key = tuple([current_field_name, temp_label]) if temp_label else tuple([current_field_name, current_field_name])\n",
    "                \n",
    "#                 check_exists_append(current_record, key, value, columns, force_new=has_nested)\n",
    "        \n",
    "#         # Only append if record has data\n",
    "#         if len(current_record) > 0:\n",
    "#             records.append(current_record)\n",
    "\n",
    "#     def process_data(item, parent_record, parent_labels):\n",
    "#         record = parent_record.copy()\n",
    "#         labels = parent_labels.copy()\n",
    "\n",
    "#         # Extract basic fields\n",
    "#         for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "#             if key in item:\n",
    "#                 record[key] = item[key]\n",
    "#                 columns.add((key, \"\"))\n",
    "\n",
    "#         # Process 'label_values'\n",
    "#         label_values = item.get('label_values', [])\n",
    "#         if label_values:\n",
    "#             for lv in label_values:\n",
    "#                 ent_field_name = lv.get('ent_field_name', '')\n",
    "#                 label = lv.get('label', '')\n",
    "#                 key_name = [ent_field_name, label] if label else [ent_field_name, ent_field_name]\n",
    "\n",
    "#                 if 'value' in lv or 'timestamp_value' in lv:\n",
    "#                     value = lv.get('value') or lv.get('timestamp_value')\n",
    "#                     if labels:\n",
    "#                         check_exists_append(record, tuple(labels), value, columns)\n",
    "#                     else:\n",
    "#                         check_exists_append(record, tuple(key_name), value, columns)\n",
    "#                 elif 'vec' in lv:\n",
    "#                     # Handle vector items recursively\n",
    "#                     process_vec_items(lv['vec'], record, key_name, labels)\n",
    "#                 elif 'dict' in lv:\n",
    "#                     # Handle dictionary items recursively\n",
    "#                     process_dict_items(lv['dict'], record, ent_field_name, \n",
    "#                                     labels if labels else key_name)\n",
    "\n",
    "#         else:\n",
    "#             # Handle case for generic JSON file\n",
    "#             for key, value in item.items():\n",
    "#                 check_exists_append(record, tuple([key]), value, columns)\n",
    "\n",
    "#         # Only append if record has data and is not already included\n",
    "#         if len(record) > 0 and record not in records:\n",
    "#             records.append(record.copy())\n",
    "\n",
    "#     # Iterate through data\n",
    "#     if isinstance(data, dict):\n",
    "#         for data_value in data.values():\n",
    "#             if isinstance(data_value, list):\n",
    "#                 for item in data_value:\n",
    "#                     process_data(item, {}, [])\n",
    "#             else:\n",
    "#                 process_data(data_value, {}, [])\n",
    "\n",
    "#     def create_multiindex_df(data):\n",
    "#         if not data:\n",
    "#             # Return empty DataFrame with proper structure\n",
    "#             return pd.DataFrame(columns=pd.MultiIndex.from_tuples([('empty', '')]))\n",
    "            \n",
    "#         # Function to flatten dictionary and preserve both tuple elements\n",
    "#         def flatten_dict(d):\n",
    "#             flattened = {}\n",
    "#             for k, v in d.items():\n",
    "#                 if isinstance(k, tuple):\n",
    "#                     # Store both parts of the tuple\n",
    "#                     flattened[k] = v\n",
    "#                 else:\n",
    "#                     # For non-tuple keys, create a tuple with same value\n",
    "#                     flattened[(k, k)] = v\n",
    "#             return flattened\n",
    "        \n",
    "#         # Flatten all dictionaries\n",
    "#         flattened_data = [flatten_dict(d) for d in data]\n",
    "        \n",
    "#         # Create initial DataFrame\n",
    "#         df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "#         if df.empty:\n",
    "#             return df\n",
    "            \n",
    "#         # Get all unique column tuples\n",
    "#         columns = df.columns.tolist()\n",
    "        \n",
    "#         # Create MultiIndex columns\n",
    "#         multi_index = pd.MultiIndex.from_tuples(columns)\n",
    "        \n",
    "#         # Create final DataFrame with MultiIndex\n",
    "#         final_df = pd.DataFrame(df.values, columns=multi_index)\n",
    "        \n",
    "#         return final_df\n",
    "    \n",
    "#     return create_multiindex_df(records)\n",
    "\n",
    "def extract_data(data, file_name = \"\"):\n",
    "    records = []\n",
    "    columns = set()\n",
    "\n",
    "    def check_exists_append(record, key, value, columns, force_new=False, is_title=False):\n",
    "        \"\"\"\n",
    "        Append function that combines values into lists unless force_new is True\n",
    "        For titles, always overwrite instead of appending\n",
    "        \"\"\"\n",
    "        if is_title:\n",
    "            record[key] = value  # Simply overwrite for titles\n",
    "            columns.add(key)\n",
    "        elif force_new:\n",
    "            record[key] = value\n",
    "            columns.add(key)\n",
    "        else:\n",
    "            if record.get(key, False):\n",
    "                if isinstance(record[key], list):\n",
    "                    record[key].append(value)\n",
    "                else:\n",
    "                    record[key] = [record[key]] + [value]\n",
    "            else:\n",
    "                record[key] = value\n",
    "                columns.add(key)\n",
    "\n",
    "    def process_vec_items(vec_items, base_record, key_name, parent_labels):\n",
    "        \"\"\"Helper function to process vector items recursively\"\"\"\n",
    "        if vec_items:\n",
    "            for nested_item in vec_items:\n",
    "                process_data({'label_values': [nested_item]}, base_record.copy(), key_name)\n",
    "\n",
    "    def has_nested_dict(dict_items):\n",
    "        \"\"\"Check if there are nested dictionaries in the dict_items\"\"\"\n",
    "        for entry in dict_items:\n",
    "            if 'dict' in entry:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def process_dict_items(dict_items, base_record, ent_field_name, labels, parent_title=None):\n",
    "        \"\"\"Helper function to process dictionary items recursively\"\"\"\n",
    "        has_nested = has_nested_dict(dict_items)\n",
    "        current_record = base_record.copy()\n",
    "        \n",
    "        # Add parent title to record if it exists\n",
    "        if parent_title:\n",
    "            title_key = tuple([ent_field_name, 'title'])\n",
    "            check_exists_append(current_record, title_key, parent_title, columns, is_title=True)\n",
    "        \n",
    "        for entry in dict_items:\n",
    "            value = entry.get('value') or entry.get('timestamp_value')\n",
    "            temp_label = entry.get('label', '')\n",
    "            nested_ent_field_name = entry.get('ent_field_name', '')\n",
    "            \n",
    "            # If there's an ent_field_name in the entry, use it\n",
    "            current_field_name = nested_ent_field_name if nested_ent_field_name else ent_field_name\n",
    "            \n",
    "            # Handle nested vectors within dictionary\n",
    "            if 'vec' in entry:\n",
    "                nested_key_name = labels + [temp_label or current_field_name]\n",
    "                process_vec_items(entry['vec'], current_record, nested_key_name, labels)\n",
    "            # Handle nested dictionaries\n",
    "            elif 'dict' in entry:\n",
    "                nested_labels = labels + [temp_label or current_field_name]\n",
    "                process_dict_items(entry['dict'], current_record, current_field_name, \n",
    "                                nested_labels, parent_title)\n",
    "            # Handle leaf values\n",
    "            else:\n",
    "                if labels:\n",
    "                    key = tuple(labels + [temp_label or current_field_name])\n",
    "                else:\n",
    "                    key = tuple([current_field_name, temp_label]) if temp_label else tuple([current_field_name, current_field_name])\n",
    "                \n",
    "                check_exists_append(current_record, key, value, columns, force_new=has_nested)\n",
    "        \n",
    "        # Only append if record has data\n",
    "        if len(current_record) > 0:\n",
    "            records.append(current_record)\n",
    "\n",
    "    def process_data(item, parent_record, parent_labels):\n",
    "        record = parent_record.copy()\n",
    "        labels = parent_labels.copy()\n",
    "\n",
    "        # Extract basic fields\n",
    "        for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "            if key in item:\n",
    "                record[key] = item[key]\n",
    "                columns.add((key, \"\"))\n",
    "\n",
    "        # Process 'label_values'\n",
    "        label_values = item.get('label_values', [])\n",
    "        if label_values:\n",
    "            for lv in label_values:\n",
    "                ent_field_name = lv.get('ent_field_name', '')\n",
    "                label = lv.get('label', '')\n",
    "                key_name = [ent_field_name, label] if label else [ent_field_name, ent_field_name]\n",
    "                \n",
    "                if 'value' in lv or 'timestamp_value' in lv:\n",
    "                    value = lv.get('value') or lv.get('timestamp_value')\n",
    "                    if labels:\n",
    "                        check_exists_append(record, tuple(labels), value, columns)\n",
    "                    else:\n",
    "                        check_exists_append(record, tuple(key_name), value, columns)\n",
    "                elif 'vec' in lv:\n",
    "                    # Handle vector items recursively\n",
    "                    process_vec_items(lv['vec'], record, key_name, labels)\n",
    "                elif 'dict' in lv:\n",
    "                    # Extract title before processing dict\n",
    "                    dict_title = lv.get('title')\n",
    "                    # Handle dictionary items recursively with title\n",
    "                    process_dict_items(lv['dict'], record, ent_field_name, \n",
    "                                    labels if labels else key_name,\n",
    "                                    dict_title)\n",
    "                    # If dict is empty but has title, create a record with just the title\n",
    "                    if not lv['dict'] and dict_title:\n",
    "                        title_record = record.copy()\n",
    "                        title_key = tuple([ent_field_name, 'title'])\n",
    "                        check_exists_append(title_record, title_key, dict_title, columns, is_title=True)\n",
    "                        records.append(title_record)\n",
    "                    continue  # Skip the final record append as it's handled in process_dict_items\n",
    "\n",
    "        else:\n",
    "            # Handle case for generic JSON file\n",
    "            for key, value in item.items():\n",
    "                check_exists_append(record, tuple([key]), value, columns)\n",
    "\n",
    "        # Only append if record has data and is not already included\n",
    "        if len(record) > 0 and record not in records:\n",
    "            records.append(record.copy())\n",
    "\n",
    "    # Iterate through data\n",
    "    if isinstance(data, dict):\n",
    "        for data_value in data.values():\n",
    "            if isinstance(data_value, list):\n",
    "                for item in data_value:\n",
    "                    process_data(item, {}, [])\n",
    "            else:\n",
    "                process_data(data_value, {}, [])\n",
    "\n",
    "    def create_multiindex_df(data):\n",
    "        if not data:\n",
    "            # Return empty DataFrame with proper structure\n",
    "            return pd.DataFrame(columns=pd.MultiIndex.from_tuples([('empty', '')]))\n",
    "            \n",
    "        # Function to flatten dictionary and preserve both tuple elements\n",
    "        def flatten_dict(d):\n",
    "            flattened = {}\n",
    "            for k, v in d.items():\n",
    "                if isinstance(k, tuple):\n",
    "                    # Store both parts of the tuple\n",
    "                    flattened[k] = v\n",
    "                else:\n",
    "                    # For non-tuple keys, create a tuple with same value\n",
    "                    flattened[(k, k)] = v\n",
    "            return flattened\n",
    "        \n",
    "        # Flatten all dictionaries\n",
    "        flattened_data = [flatten_dict(d) for d in data]\n",
    "        \n",
    "        # Create initial DataFrame\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        # Get all unique column tuples\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        # Create MultiIndex columns\n",
    "        multi_index = pd.MultiIndex.from_tuples(columns)\n",
    "        \n",
    "        # Create final DataFrame with MultiIndex\n",
    "        final_df = pd.DataFrame(df.values, columns=multi_index)\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    return create_multiindex_df(records)\n",
    "\n",
    "\n",
    "def convert_to_pkl(root_dir):\n",
    "    assert not root_dir.endswith('.json'), f\"ERROR: Path must be a directory not a file {root_dir}\"\n",
    "    root_temp = \"\"\n",
    "    folder_loc = -1\n",
    "    new_name = \"default_csv\"\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        # On first iteration we're setting up temp variables\n",
    "        if not root_temp:\n",
    "            root_temp = root.split(\"/\")\n",
    "            folder_loc = len(root_temp) - 1\n",
    "            new_name = root_temp[folder_loc] + \"_pkl_converted\"\n",
    "        \n",
    "        # For each file in each directory\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                # Create new output path\n",
    "                output_root = root.split(\"/\")\n",
    "                output_root[folder_loc] = new_name\n",
    "                output_root = \"/\".join(output_root)\n",
    "                \n",
    "                # Process the file\n",
    "                filepath = os.path.join(root, file)\n",
    "                print(f\"Processing {filepath}\")\n",
    "                \n",
    "                temp_data = read_json_files(filepath)\n",
    "                temp_records = extract_data(temp_data, file)\n",
    "                \n",
    "                # Create output directory if it doesn't exist\n",
    "                os.makedirs(output_root, exist_ok=True)\n",
    "                \n",
    "                # Save each DataFrame in the dictionary\n",
    "                output_filename = file.replace('.json', '.pkl')\n",
    "                output_csv_filename = file.replace('.json', '.csv')\n",
    "                output_path = os.path.join(output_root, output_filename)\n",
    "                output_csv_path = os.path.join(output_root, output_csv_filename)\n",
    "                temp_records.to_pickle(output_path, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                temp_records.to_csv(output_csv_path)\n",
    "                print(f\"Saved {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_data = read_json_files('Workplace Data Company Information/organization/company_info_1.json')\n",
    "df = extract_data(temp_data)\n",
    "df.to_csv('company_info_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def extract_company_info(data_dict):\n",
    "    \"\"\"\n",
    "    Extracts company information from a JSON structure.\n",
    "    Long description fields are not used as keys.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    # Build a template from the top-level keys (except 'label_values')\n",
    "    record_template = {(key, key): value for key, value in data_dict.items() if key != 'label_values'}\n",
    "    \n",
    "    # For non-nested (simple) entries, update the template.\n",
    "    for item in data_dict.get('label_values', []):\n",
    "        if \"dict\" not in item and \"vec\" not in item:\n",
    "            record_template[(item['ent_field_name'], item.get('label', np.nan))] = item.get('value', np.nan)\n",
    "    \n",
    "    # Handle nested entries (records with a \"dict\" key)\n",
    "    for item in data_dict.get('label_values', []):\n",
    "        if \"dict\" not in item or \"vec\" not in item:\n",
    "            continue\n",
    "        base_record = deepcopy(record_template)\n",
    "        # Check if the nested entries are grouped (e.g., group installs) by looking for a nested \"dict\" inside.\n",
    "        first_nested = item['dict'][0] if item['dict'] else None\n",
    "        if first_nested and isinstance(first_nested, dict) and 'dict' in first_nested:\n",
    "            # This is a group of nested records\n",
    "            for group_item in item['dict']:\n",
    "                record = deepcopy(base_record)\n",
    "                if 'title' in group_item:\n",
    "                    record[(\"title\", \"title\")] = group_item['title']\n",
    "                for sub_entry in group_item.get('dict', []):\n",
    "                    record[(sub_entry['ent_field_name'], sub_entry.get('label', sub_entry['ent_field_name']))] = sub_entry.get('value', np.nan)\n",
    "                records.append(record)\n",
    "        else:\n",
    "            # Otherwise, the nested dict is at one level.\n",
    "            record = deepcopy(base_record)\n",
    "            for sub_entry in item['dict']:\n",
    "                record[(sub_entry['ent_field_name'], sub_entry.get('label', sub_entry['ent_field_name']))] = sub_entry.get('value', np.nan)\n",
    "            records.append(record)\n",
    "    \n",
    "    if records:\n",
    "        column_index = pd.MultiIndex.from_tuples(list(records[0].keys()))\n",
    "        df = pd.DataFrame.from_records(records, columns=column_index)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "with open('Workplace Data Company Information/organization/company_info_1.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = extract_company_info(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>media</th>\n",
       "      <th>fbid</th>\n",
       "      <th>ent_name</th>\n",
       "      <th>BlockNotificationsNonCorpEmails</th>\n",
       "      <th>PrimaryPointOfContactID</th>\n",
       "      <th>OnlyAdminsCanCreateMultiCompanyGroups</th>\n",
       "      <th>BlockMultiCompanyGroupInvites</th>\n",
       "      <th>GuestInviteSettings</th>\n",
       "      <th>GuestInviteSelectorSettings</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"8\" halign=\"left\"></th>\n",
       "      <th>GroupID</th>\n",
       "      <th>GroupTimestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>media</th>\n",
       "      <th>fbid</th>\n",
       "      <th>ent_name</th>\n",
       "      <th>Block Notifs non corp emails</th>\n",
       "      <th>Primary point of contact</th>\n",
       "      <th>Only admins can create multi company groups</th>\n",
       "      <th>Block multi company group invites</th>\n",
       "      <th>Guest invite settings</th>\n",
       "      <th>Guest invite selector settings</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"8\" halign=\"left\"></th>\n",
       "      <th>GroupID</th>\n",
       "      <th>GroupTimestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"8\" halign=\"left\"></th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>...</th>\n",
       "      <th>WebhooksSettings</th>\n",
       "      <th>SecuritySettings</th>\n",
       "      <th>LinkPreviewSettings</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Allowed groups</th>\n",
       "      <th>Primary email</th>\n",
       "      <th>Name</th>\n",
       "      <th>Membership type</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>...</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>SelectedGroupID</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>...</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1460959825</td>\n",
       "      <td>[]</td>\n",
       "      <td>229949230696345</td>\n",
       "      <td>EntScimCompany</td>\n",
       "      <td>False</td>\n",
       "      <td>100011662284821</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1460959825</td>\n",
       "      <td>[]</td>\n",
       "      <td>229949230696345</td>\n",
       "      <td>EntScimCompany</td>\n",
       "      <td>False</td>\n",
       "      <td>100011662284821</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1460959825</td>\n",
       "      <td>[]</td>\n",
       "      <td>229949230696345</td>\n",
       "      <td>EntScimCompany</td>\n",
       "      <td>False</td>\n",
       "      <td>100011662284821</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1460959825</td>\n",
       "      <td>[]</td>\n",
       "      <td>229949230696345</td>\n",
       "      <td>EntScimCompany</td>\n",
       "      <td>False</td>\n",
       "      <td>100011662284821</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1460959825</td>\n",
       "      <td>[]</td>\n",
       "      <td>229949230696345</td>\n",
       "      <td>EntScimCompany</td>\n",
       "      <td>False</td>\n",
       "      <td>100011662284821</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1460959825</td>\n",
       "      <td>[]</td>\n",
       "      <td>229949230696345</td>\n",
       "      <td>EntScimCompany</td>\n",
       "      <td>False</td>\n",
       "      <td>100011662284821</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1460959825</td>\n",
       "      <td>[]</td>\n",
       "      <td>229949230696345</td>\n",
       "      <td>EntScimCompany</td>\n",
       "      <td>False</td>\n",
       "      <td>100011662284821</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1460959825</td>\n",
       "      <td>[]</td>\n",
       "      <td>229949230696345</td>\n",
       "      <td>EntScimCompany</td>\n",
       "      <td>False</td>\n",
       "      <td>100011662284821</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1460959825</td>\n",
       "      <td>[]</td>\n",
       "      <td>229949230696345</td>\n",
       "      <td>EntScimCompany</td>\n",
       "      <td>False</td>\n",
       "      <td>100011662284821</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1460959825</td>\n",
       "      <td>[]</td>\n",
       "      <td>229949230696345</td>\n",
       "      <td>EntScimCompany</td>\n",
       "      <td>False</td>\n",
       "      <td>100011662284821</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229949227363012</td>\n",
       "      <td>1460959824.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>417 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp media             fbid        ent_name  \\\n",
       "      timestamp media             fbid        ent_name   \n",
       "            NaN   NaN              NaN             NaN   \n",
       "            NaN   NaN              NaN             NaN   \n",
       "            NaN   NaN              NaN             NaN   \n",
       "            NaN   NaN              NaN             NaN   \n",
       "0    1460959825    []  229949230696345  EntScimCompany   \n",
       "1    1460959825    []  229949230696345  EntScimCompany   \n",
       "2    1460959825    []  229949230696345  EntScimCompany   \n",
       "3    1460959825    []  229949230696345  EntScimCompany   \n",
       "4    1460959825    []  229949230696345  EntScimCompany   \n",
       "..          ...   ...              ...             ...   \n",
       "412  1460959825    []  229949230696345  EntScimCompany   \n",
       "413  1460959825    []  229949230696345  EntScimCompany   \n",
       "414  1460959825    []  229949230696345  EntScimCompany   \n",
       "415  1460959825    []  229949230696345  EntScimCompany   \n",
       "416  1460959825    []  229949230696345  EntScimCompany   \n",
       "\n",
       "    BlockNotificationsNonCorpEmails  PrimaryPointOfContactID  \\\n",
       "       Block Notifs non corp emails Primary point of contact   \n",
       "                                NaN                      NaN   \n",
       "                                NaN                      NaN   \n",
       "                                NaN                      NaN   \n",
       "                                NaN                      NaN   \n",
       "0                             False          100011662284821   \n",
       "1                             False          100011662284821   \n",
       "2                             False          100011662284821   \n",
       "3                             False          100011662284821   \n",
       "4                             False          100011662284821   \n",
       "..                              ...                      ...   \n",
       "412                           False          100011662284821   \n",
       "413                           False          100011662284821   \n",
       "414                           False          100011662284821   \n",
       "415                           False          100011662284821   \n",
       "416                           False          100011662284821   \n",
       "\n",
       "          OnlyAdminsCanCreateMultiCompanyGroups  \\\n",
       "    Only admins can create multi company groups   \n",
       "                                            NaN   \n",
       "                                            NaN   \n",
       "                                            NaN   \n",
       "                                            NaN   \n",
       "0                                          True   \n",
       "1                                          True   \n",
       "2                                          True   \n",
       "3                                          True   \n",
       "4                                          True   \n",
       "..                                          ...   \n",
       "412                                        True   \n",
       "413                                        True   \n",
       "414                                        True   \n",
       "415                                        True   \n",
       "416                                        True   \n",
       "\n",
       "        BlockMultiCompanyGroupInvites   GuestInviteSettings  \\\n",
       "    Block multi company group invites Guest invite settings   \n",
       "                                  NaN                   NaN   \n",
       "                                  NaN                   NaN   \n",
       "                                  NaN                   NaN   \n",
       "                                  NaN                   NaN   \n",
       "0                               False                  None   \n",
       "1                               False                  None   \n",
       "2                               False                  None   \n",
       "3                               False                  None   \n",
       "4                               False                  None   \n",
       "..                                ...                   ...   \n",
       "412                             False                  None   \n",
       "413                             False                  None   \n",
       "414                             False                  None   \n",
       "415                             False                  None   \n",
       "416                             False                  None   \n",
       "\n",
       "       GuestInviteSelectorSettings  ...                                    \\\n",
       "    Guest invite selector settings  ...                                     \n",
       "                               NaN  ...                                     \n",
       "                               NaN  ... WebhooksSettings SecuritySettings   \n",
       "                               NaN  ...              NaN              NaN   \n",
       "                               NaN  ...              NaN              NaN   \n",
       "0                             None  ...              NaN              NaN   \n",
       "1                             None  ...              NaN              NaN   \n",
       "2                             None  ...              NaN              NaN   \n",
       "3                             None  ...              NaN              NaN   \n",
       "4                             None  ...              NaN              NaN   \n",
       "..                             ...  ...              ...              ...   \n",
       "412                           None  ...              NaN              NaN   \n",
       "413                           None  ...              NaN              NaN   \n",
       "414                           None  ...              NaN              NaN   \n",
       "415                           None  ...              NaN              NaN   \n",
       "416                           None  ...              NaN              NaN   \n",
       "\n",
       "                                                                 \\\n",
       "                                                                  \n",
       "                                                                  \n",
       "    LinkPreviewSettings  Allowed groups      Primary email Name   \n",
       "                    NaN SelectedGroupID  NaN           NaN  NaN   \n",
       "                    NaN             NaN  NaN           NaN  NaN   \n",
       "0                   NaN             NaN  NaN           NaN  NaN   \n",
       "1                   NaN             NaN  NaN           NaN  NaN   \n",
       "2                   NaN             NaN  NaN           NaN  NaN   \n",
       "3                   NaN             NaN  NaN           NaN  NaN   \n",
       "4                   NaN             NaN  NaN           NaN  NaN   \n",
       "..                  ...             ...  ...           ...  ...   \n",
       "412                 NaN             NaN  NaN           NaN  NaN   \n",
       "413                 NaN             NaN  NaN           NaN  NaN   \n",
       "414                 NaN             NaN  NaN           NaN  NaN   \n",
       "415                 NaN             NaN  NaN           NaN  NaN   \n",
       "416                 NaN             NaN  NaN           NaN  NaN   \n",
       "\n",
       "                             GroupID GroupTimestamp  \n",
       "                             GroupID GroupTimestamp  \n",
       "                                 NaN            NaN  \n",
       "    Membership type              NaN            NaN  \n",
       "                NaN              NaN            NaN  \n",
       "                NaN              NaN            NaN  \n",
       "0               NaN              NaN            NaN  \n",
       "1               NaN              NaN            NaN  \n",
       "2               NaN              NaN            NaN  \n",
       "3               NaN              NaN            NaN  \n",
       "4               NaN              NaN            NaN  \n",
       "..              ...              ...            ...  \n",
       "412             NaN              NaN            NaN  \n",
       "413             NaN              NaN            NaN  \n",
       "414             NaN              NaN            NaN  \n",
       "415             NaN              NaN            NaN  \n",
       "416             NaN  229949227363012   1460959824.0  \n",
       "\n",
       "[417 rows x 119 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract and view single file\n",
    "temp_data = read_json_files('Workplace Data Company Information/organization/company_info_1.json')\n",
    "temp_records = extract_data(temp_data)\n",
    "temp_records.to_csv('company_info_1.csv')\n",
    "temp_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert whole directory to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Workplace Data Company Information_pkl_converted/manifest.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/post_campaigns_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_contact_support_cases_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_6.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/promoted_post_notice_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_info_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_sso_settings_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/employee_safety_checkup_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/security_logs_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/email_domain_settings_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_7.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_level_installed_apps_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/community_events_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/workplace_team_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_reported_content_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/custom_badges_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_reported_content_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/work_invites_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/email_domains_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/crisis_information_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/security_logs_4.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_3.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/security_logs_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_4.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_info_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/bulk_tasks_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_8.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/profile_field_sets_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_9.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/promoted_post_notice_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_contact_support_cases_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_5.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/security_logs_3.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_6.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_10.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_7.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_8.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_4.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_5.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_9.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_3.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_3.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/people_sets_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_11.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_8.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_31.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_27.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_4.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_5.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_26.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_30.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_10.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_9.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_37.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_21.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_5.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_17.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_40.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_41.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_16.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_4.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_3.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_20.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_8.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_36.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_19.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_23.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_35.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_42.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_15.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_7.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_39.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_38.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_6.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_14.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_43.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_34.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_22.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_18.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_44.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_13.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_29.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_6.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_25.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_33.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_32.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_24.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_7.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_28.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_12.pkl\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'Workplace Data Company Information'\n",
    "convert_to_pkl(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workplace-converter-FMkCQOQs-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
