{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_strings_in_json(directory, strings_to_find):\n",
    "    matches = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        content = f.read()\n",
    "                        for string in strings_to_find:\n",
    "                            if string in content.lower():\n",
    "                                matches.append((filepath, string))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {filepath}: {e}\")\n",
    "    return matches\n",
    "\n",
    "# strings_to_find = ['2307148732639320', '2090275401330376', '2090275404663709', '100056956201608']\n",
    "strings_to_find=['8073719999360913']\n",
    "\n",
    "#company info and company group:61565211886319 --> people_sets_1 & child_groups, 1910535509304367\n",
    "\n",
    "#7775445322516960\n",
    "#565159963841935\n",
    "#1910351919322726\n",
    "#100040716736805\n",
    "# directory = '/Users/dan/Documents/local_code/test/data/woolies/workplace/organization'\n",
    "# directory = '/Users/dan/Documents/local_code/test/data/woolies/workplace/groups'\n",
    "# directory = 'Workplace Data Company Information/groups'\n",
    "# directory = 'Workplace Data Company Information/organization'\n",
    "# directory = 'Workplace Data Company Information/user_profiles'\n",
    "directory = 'Workplace Data Company Information/groups'\n",
    "matches = search_strings_in_json(directory, strings_to_find)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for match in matches:\n",
    "    count+=1\n",
    "    print(f\"Found '{match[1]}' in file: {match[0]}\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON to pkl converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "def read_json_files(root_dir):\n",
    "    data = {}\n",
    "    if root_dir.endswith('.json'):\n",
    "        temp_key_name = root_dir.split('/')[-1].split('.')[0]\n",
    "        with open(root_dir, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "                data[temp_key_name] = json_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error reading {root_dir}: {e}\")\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.json'):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        temp_key_name = root_dir + '/' + file\n",
    "                        try:\n",
    "                            json_data = json.load(f)\n",
    "                            data[temp_key_name] = json_data\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error reading {filepath}: {e}\")\n",
    "    return data\n",
    "\n",
    "def extract_company_group(data_dict):\n",
    "    data_dict = list(data_dict.values())[0]\n",
    "    records = []\n",
    "    record_template = {(key, key): value for key, value in data_dict.items() if key != 'label_values'}\n",
    "    \n",
    "    \n",
    "\n",
    "    from copy import deepcopy\n",
    "\n",
    "    for item in data_dict['label_values']:\n",
    "        # print(item)\n",
    "        if not \"dict\" in item:\n",
    "            record_template[(item['ent_field_name'], item['label'])] = item.get('value', np.nan)\n",
    "\n",
    "    for item in data_dict['label_values']:\n",
    "        if \"dict\" not in item: continue\n",
    "\n",
    "        record = deepcopy(record_template)\n",
    "        # print(item)\n",
    "        record[(\"title\", \"title\")] = item['title']\n",
    "        record_copy = deepcopy(record)\n",
    "        \n",
    "        entry_list = item['dict']\n",
    "\n",
    "        for entry in entry_list:\n",
    "            for s_entry in entry['dict']:\n",
    "                # print(s_entry)\n",
    "\n",
    "                record_copy[(s_entry['ent_field_name'], s_entry.get('label', s_entry['ent_field_name']))] = s_entry['value']\n",
    "\n",
    "            records.append(deepcopy(record_copy))\n",
    "\n",
    "    column_index = pd.MultiIndex.from_tuples(list(records[0].keys()))\n",
    "    df = pd.DataFrame.from_records(records, columns=column_index)\n",
    "\n",
    "    # print(records)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_data_old(data, file_name = \"\"):\n",
    "    records = []\n",
    "    columns = set()\n",
    "\n",
    "    def check_exists_append(record, key, value, columns):\n",
    "        if record.get(key, False):\n",
    "            if isinstance(record[key], list):\n",
    "                record[key].append(value)\n",
    "            else:\n",
    "                record[key] = [record[key]] + [value]\n",
    "        else:\n",
    "            record[key] = value\n",
    "            columns.add(key)\n",
    "\n",
    "    def process_vec_items(vec_items, record, key_name, parent_labels):\n",
    "        \"\"\"Helper function to process vector items recursively\"\"\"\n",
    "        if vec_items:\n",
    "            for nested_item in vec_items:\n",
    "                process_data({'label_values': [nested_item]}, record, key_name)\n",
    "\n",
    "    def process_dict_items(dict_items, record, ent_field_name, labels):\n",
    "        \"\"\"Helper function to process dictionary items recursively\"\"\"\n",
    "        for entry in dict_items:\n",
    "            value = entry.get('value') or entry.get('timestamp_value')\n",
    "            temp_label = entry.get('label', '')\n",
    "            \n",
    "            # Handle nested vectors within dictionary\n",
    "            if 'vec' in entry:\n",
    "                nested_key_name = labels + [temp_label] if temp_label else labels + [ent_field_name]\n",
    "                process_vec_items(entry['vec'], record, nested_key_name, labels)\n",
    "            # Handle nested dictionaries\n",
    "            elif 'dict' in entry:\n",
    "                nested_labels = labels + [temp_label] if temp_label else labels + [ent_field_name]\n",
    "                process_dict_items(entry['dict'], record, ent_field_name, nested_labels)\n",
    "            # Handle leaf values\n",
    "            else:\n",
    "                if labels:\n",
    "                    if temp_label:\n",
    "                        check_exists_append(record, tuple(labels + [temp_label]), value, columns)\n",
    "                    else:\n",
    "                        check_exists_append(record, tuple(labels + [ent_field_name]), value, columns)\n",
    "                else:\n",
    "                    check_exists_append(record, tuple([ent_field_name, temp_label]), value, columns)\n",
    "\n",
    "    def process_data(item, parent_record, parent_labels):\n",
    "        new_record = 0\n",
    "        record = parent_record.copy()\n",
    "        labels = parent_labels.copy()\n",
    "\n",
    "        # Make a new entry if we're at the top level\n",
    "        for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "            if key in item:\n",
    "                new_record = 1\n",
    "\n",
    "        if new_record == 1:\n",
    "            record = {}.copy()\n",
    "\n",
    "        # Extract basic fields\n",
    "        for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "            if key in item:\n",
    "                record[key] = item[key]\n",
    "                columns.add((key, \"\"))\n",
    "\n",
    "        # Process 'label_values'\n",
    "        label_values = item.get('label_values', [])\n",
    "        if label_values:\n",
    "            for lv in label_values:\n",
    "                ent_field_name = lv.get('ent_field_name', '')\n",
    "                label = lv.get('label', '')\n",
    "                key_name = [ent_field_name, label]\n",
    "\n",
    "                if 'value' in lv or 'timestamp_value' in lv:\n",
    "                    value = lv.get('value') or lv.get('timestamp_value')\n",
    "                    if labels:\n",
    "                        check_exists_append(record, tuple(labels), value, columns)\n",
    "                    else:\n",
    "                        check_exists_append(record, tuple(key_name), value, columns)\n",
    "                elif 'vec' in lv:\n",
    "                    # Handle vector items recursively\n",
    "                    process_vec_items(lv['vec'], record, key_name, labels)\n",
    "                elif 'dict' in lv:\n",
    "                    # Handle dictionary items recursively\n",
    "                    process_dict_items(lv['dict'], record, ent_field_name, \n",
    "                                    labels if labels else key_name)\n",
    "\n",
    "        else:\n",
    "            # Handle case for generic JSON file\n",
    "            for key, value in item.items():\n",
    "                check_exists_append(record, tuple([key]), value, columns)\n",
    "\n",
    "        records.append(record.copy())\n",
    "\n",
    "    # Iterate through a list\n",
    "    for data_value in data.values():\n",
    "        if isinstance(data_value, list):\n",
    "            for item in data_value:\n",
    "                process_data(item, {}, [])\n",
    "        else:\n",
    "            process_data(data_value, {}, [])\n",
    "\n",
    "    def create_multiindex_df(data):\n",
    "        # Function to flatten dictionary and preserve both tuple elements\n",
    "        def flatten_dict(d):\n",
    "            flattened = {}\n",
    "            for k, v in d.items():\n",
    "                if isinstance(k, tuple):\n",
    "                    # Store both parts of the tuple\n",
    "                    flattened[k] = v\n",
    "                else:\n",
    "                    # For non-tuple keys, create a tuple with same value\n",
    "                    flattened[(k, k)] = v\n",
    "            return flattened\n",
    "        \n",
    "        # Flatten all dictionaries\n",
    "        flattened_data = [flatten_dict(d) for d in data]\n",
    "        \n",
    "        # Create initial DataFrame\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "        # Get all unique column tuples\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        # Create MultiIndex columns\n",
    "        multi_index = pd.MultiIndex.from_tuples(columns)\n",
    "        \n",
    "        # Create final DataFrame with MultiIndex\n",
    "        final_df = pd.DataFrame(df.values, columns=multi_index)\n",
    "\n",
    "        def remove_duplicates_with_lists(df):\n",
    "            # Convert lists to strings for comparison\n",
    "            df_comparable = df.copy()\n",
    "            \n",
    "            # Convert empty lists to a string representation\n",
    "            df_comparable = df_comparable.applymap(lambda x: str(x) if isinstance(x, list) else x)\n",
    "            \n",
    "            # Drop duplicates based on all columns except the first one (assuming first is index)\n",
    "            # Keep first occurrence and preserve order\n",
    "            unique_indices = df_comparable.iloc[:, 1:].drop_duplicates().index\n",
    "            \n",
    "            # Return original dataframe with duplicates removed\n",
    "            return df.iloc[unique_indices]\n",
    "        \n",
    "        final_df_no_duplicates = remove_duplicates_with_lists(final_df)\n",
    "        \n",
    "        return final_df_no_duplicates\n",
    "    \n",
    "    return create_multiindex_df(records)\n",
    "\n",
    "\n",
    "def is_redundant_record(prev_record, current_record):\n",
    "    \"\"\"Check if the current record is redundant with the previous record\"\"\"\n",
    "    if not prev_record or not current_record:\n",
    "        return False\n",
    "    for key, value in current_record.items():\n",
    "        if not value:\n",
    "            continue\n",
    "        if key not in prev_record or prev_record[key] != value:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def extract_data(data, file_name = \"\"):\n",
    "    records = []\n",
    "    columns = set()\n",
    "\n",
    "    def check_exists_append(record, key, value, columns, force_new=False, is_title=False):\n",
    "        \"\"\"\n",
    "        Append function that combines values into lists unless force_new is True\n",
    "        For titles, always overwrite instead of appending\n",
    "        \"\"\"\n",
    "        if is_title:\n",
    "            record[key] = value  # Simply overwrite for titles\n",
    "            columns.add(key)\n",
    "        elif force_new:\n",
    "            record[key] = value\n",
    "            columns.add(key)\n",
    "        else:\n",
    "            if record.get(key, False):\n",
    "                if isinstance(record[key], list):\n",
    "                    record[key].append(value)\n",
    "                else:\n",
    "                    record[key] = [record[key]] + [value]\n",
    "            else:\n",
    "                record[key] = value\n",
    "                columns.add(key)\n",
    "\n",
    "    def process_vec_items(vec_items, base_record, key_name, parent_labels):\n",
    "        \"\"\"Helper function to process vector items recursively\"\"\"\n",
    "        if vec_items:\n",
    "            for nested_item in vec_items:\n",
    "                process_data({'label_values': [nested_item]}, base_record.copy(), key_name)\n",
    "\n",
    "    def process_dict_items(dict_items, base_record, ent_field_name, labels, parent_title=None, depth=0):\n",
    "        \"\"\"Helper function to process dictionary items recursively\"\"\"\n",
    "        # if not has_nested:\n",
    "        #     has_nested = has_nested_dict(dict_items)\n",
    "        current_record = base_record if depth == 0 else base_record.copy()\n",
    "        \n",
    "        # Add parent title to record if it exists\n",
    "        if parent_title:\n",
    "            title_key = tuple([ent_field_name, 'title'])\n",
    "            check_exists_append(current_record, title_key, parent_title, columns, is_title=True)\n",
    "        \n",
    "        for entry in dict_items:\n",
    "            value = entry.get('value') or entry.get('timestamp_value')\n",
    "            temp_label = entry.get('label', '')\n",
    "            nested_ent_field_name = entry.get('ent_field_name', '')\n",
    "            \n",
    "            # If there's an ent_field_name in the entry, use it\n",
    "            current_field_name = nested_ent_field_name if nested_ent_field_name else ent_field_name\n",
    "            \n",
    "            # Handle nested vectors within dictionary\n",
    "            if 'vec' in entry:\n",
    "                nested_key_name = labels + [temp_label or current_field_name]\n",
    "                process_vec_items(entry['vec'], current_record, nested_key_name, labels)\n",
    "            # Handle nested dictionaries\n",
    "            elif 'dict' in entry:\n",
    "                nested_labels = labels + [temp_label or current_field_name]\n",
    "                process_dict_items(entry['dict'], current_record, current_field_name, \n",
    "                                nested_labels, parent_title, depth=depth+1)\n",
    "            # Handle leaf values\n",
    "            else:\n",
    "                if labels:\n",
    "                    key = tuple(labels + [temp_label or current_field_name])\n",
    "                else:\n",
    "                    key = tuple([current_field_name, temp_label]) if temp_label else tuple([current_field_name, current_field_name])\n",
    "                \n",
    "                check_exists_append(current_record, key, value, columns, force_new=depth > 0)\n",
    "        \n",
    "        # Only append if record has data\n",
    "        if depth > 0 and current_record not in records and dict_items:\n",
    "            records.append(current_record)\n",
    "\n",
    "    def process_data(item, parent_record, parent_labels):\n",
    "        record = parent_record.copy()\n",
    "        labels = parent_labels.copy()\n",
    "\n",
    "        # Extract basic fields\n",
    "        for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "            if key in item:\n",
    "                record[key] = item[key]\n",
    "                columns.add((key, \"\"))\n",
    "\n",
    "        # Process 'label_values'\n",
    "        label_values = item.get('label_values', [])\n",
    "        if label_values:\n",
    "            for lv in label_values:\n",
    "                ent_field_name = lv.get('ent_field_name', '')\n",
    "                label = lv.get('label', '')\n",
    "                key_name = [ent_field_name, label] if label else [ent_field_name, ent_field_name]\n",
    "                \n",
    "                if 'value' in lv or 'timestamp_value' in lv:\n",
    "                    value = lv.get('value') or lv.get('timestamp_value')\n",
    "                    if labels:\n",
    "                        check_exists_append(record, tuple(labels), value, columns)\n",
    "                    else:\n",
    "                        check_exists_append(record, tuple(key_name), value, columns)\n",
    "                elif 'vec' in lv:\n",
    "                    # Handle vector items recursively\n",
    "                    process_vec_items(lv['vec'], record, key_name, labels)\n",
    "                elif 'dict' in lv:\n",
    "                    # Extract title before processing dict\n",
    "                    dict_title = lv.get('title')\n",
    "                    # Handle dictionary items recursively with title\n",
    "                    process_dict_items(lv['dict'], record, ent_field_name, \n",
    "                                    labels if labels else key_name,\n",
    "                                    dict_title)\n",
    "                    \n",
    "        else:\n",
    "            # Handle case for generic JSON file\n",
    "            for key, value in item.items():\n",
    "                check_exists_append(record, tuple([key]), value, columns)\n",
    "\n",
    "        if len(records) > 0 and is_redundant_record(records[-1], record):\n",
    "            return\n",
    "\n",
    "        # Only append if record has data and is not already included\n",
    "        if len(record) > 0:\n",
    "            records.append(record.copy())\n",
    "\n",
    "    # Iterate through data\n",
    "    if isinstance(data, dict):\n",
    "        for data_value in data.values():\n",
    "            if isinstance(data_value, list):\n",
    "                for item in data_value:\n",
    "                    process_data(item, {}, [])\n",
    "            else:\n",
    "                process_data(data_value, {}, [])\n",
    "\n",
    "    def create_multiindex_df(data):\n",
    "        if not data:\n",
    "            # Return empty DataFrame with proper structure\n",
    "            return pd.DataFrame(columns=pd.MultiIndex.from_tuples([('empty', '')]))\n",
    "            \n",
    "        # Function to flatten dictionary and preserve both tuple elements\n",
    "        def flatten_dict(d):\n",
    "            flattened = {}\n",
    "            for k, v in d.items():\n",
    "                if isinstance(k, tuple):\n",
    "                    # Store both parts of the tuple\n",
    "                    flattened[k] = v\n",
    "                else:\n",
    "                    # For non-tuple keys, create a tuple with same value\n",
    "                    flattened[(k, k)] = v\n",
    "            return flattened\n",
    "        \n",
    "        # Flatten all dictionaries\n",
    "        flattened_data = [flatten_dict(d) for d in data]\n",
    "        \n",
    "        # Create initial DataFrame\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        # Get all unique column tuples\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        # Create MultiIndex columns\n",
    "        multi_index = pd.MultiIndex.from_tuples(columns)\n",
    "        \n",
    "        # Create final DataFrame with MultiIndex\n",
    "        final_df = pd.DataFrame(df.values, columns=multi_index)\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    return create_multiindex_df(records)\n",
    "\n",
    "\n",
    "def convert_to_pkl(root_dir):\n",
    "    assert not root_dir.endswith('.json'), f\"ERROR: Path must be a directory not a file {root_dir}\"\n",
    "    root_temp = \"\"\n",
    "    folder_loc = -1\n",
    "    new_name = \"default_csv\"\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        # On first iteration we're setting up temp variables\n",
    "        if not root_temp:\n",
    "            root_temp = root.split(\"/\")\n",
    "            folder_loc = len(root_temp) - 1\n",
    "            new_name = root_temp[folder_loc] + \"_pkl_converted\"\n",
    "        \n",
    "        # For each file in each directory\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                # Create new output path\n",
    "                output_root = root.split(\"/\")\n",
    "                output_root[folder_loc] = new_name\n",
    "                output_root = \"/\".join(output_root)\n",
    "                \n",
    "                # Process the file\n",
    "                filepath = os.path.join(root, file)\n",
    "                print(f\"Processing {filepath}\")\n",
    "                \n",
    "                temp_data = read_json_files(filepath)\n",
    "                temp_records = extract_data(temp_data, file)\n",
    "                \n",
    "                # Create output directory if it doesn't exist\n",
    "                os.makedirs(output_root, exist_ok=True)\n",
    "                \n",
    "                # Save each DataFrame in the dictionary\n",
    "                output_filename = file.replace('.json', '.pkl')\n",
    "                output_csv_filename = file.replace('.json', '.csv')\n",
    "                output_path = os.path.join(output_root, output_filename)\n",
    "                output_csv_path = os.path.join(output_root, output_csv_filename)\n",
    "                temp_records.to_pickle(output_path, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                temp_records.to_csv(output_csv_path)\n",
    "                print(f\"Saved {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and view single file\n",
    "temp_data = read_json_files('Workplace Data Company Information/groups/people_sets_1.json')\n",
    "temp_records = extract_data(temp_data)\n",
    "temp_records.to_csv('company_info_1.csv')\n",
    "temp_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert whole directory to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'Workplace Data Company Information'\n",
    "convert_to_pkl(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from pkl\n",
    "df = pd.read_pickle('Workplace Data Company Information_pkl_converted/groups/child_groups_1.pkl')\n",
    "df['media', 'media', np.nan, np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymize_columns = {\n",
    "    \"Workplace Data Company Information_pkl_converted/groups/child_groups_\": [\n",
    "        ('', '', '', 'Primary email'),\n",
    "        ('', '', '', 'Name')\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/groups/multi_company_group_\": [\n",
    "        ('', '', '', 'Primary email'),\n",
    "        ('', '', '', 'Name')\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/organization/company_contact_support_cases_1\": [\n",
    "        (\"CaseExtraData\", \"Case extra data\", \"Affected Users\", np.nan, np.nan, np.nan),\n",
    "        (\"CaseExtraData\", \"Case extra data\", \"Email addresses of the affected users\", np.nan, np.nan, np.nan)\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/organization/company_contact_support_cases_2\": [\n",
    "        (\"CaseExtraData\", \"Case extra data\", \"Email addresses of the affected users\", np.nan, np.nan, np.nan)\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/organization/company_group_\": [\n",
    "        ('', '', '', 'Primary email'),\n",
    "        ('', '', '', 'Name')\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/organization/company_info_1\": [\n",
    "        ('', '', '', 'Primary email', np.nan, np.nan),\n",
    "        ('', '', '', 'Name', np.nan, np.nan)\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/organization/employee_safety_checkup_1\": [\n",
    "        (\"Email\", \"Email\")\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/organization/security_logs_\": [\n",
    "        (\"IP\", \"IP\")\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/organization/work_invites_1\": [\n",
    "        (\"ContactRaw\", \"Contact\")\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/user_profiles/company_members_1\":[\n",
    "        (\"NormalizedUserNameRaw\", \"Username\", np.nan, np.nan),\n",
    "        (\"FormattedNameRaw\", \"Name\", np.nan, np.nan),\n",
    "        (\"FamilyName\", \"Family name\", np.nan, np.nan),\n",
    "        (\"GivenName\", \"Given name\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Email address\"),\n",
    "        (\"Gender\", \"Gender\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"UnformattedNumber\"),\n",
    "        (\"\", \"\", \"\", \"Phone numbers\")\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/user_profiles/company_members_2\":[\n",
    "        (\"NormalizedUserNameRaw\", \"Username\", np.nan, np.nan),\n",
    "        (\"FormattedNameRaw\", \"Name\", np.nan, np.nan),\n",
    "        (\"FamilyName\", \"Family name\", np.nan, np.nan),\n",
    "        (\"GivenName\", \"Given name\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Email address\"),\n",
    "        (\"Gender\", \"Gender\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"UnformattedNumber\"),\n",
    "        (\"\", \"\", \"\", \"Phone numbers\")\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/user_profiles/company_members_3\":[\n",
    "        (\"NormalizedUserNameRaw\", \"Username\", np.nan, np.nan),\n",
    "        (\"FormattedNameRaw\", \"Name\", np.nan, np.nan),\n",
    "        (\"FamilyName\", \"Family name\", np.nan, np.nan),\n",
    "        (\"GivenName\", \"Given name\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Email address\"),\n",
    "        (\"\", \"\", \"\", \"UnformattedNumber\"),\n",
    "        (\"\", \"\", \"\", \"Phone numbers\")\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/user_profiles/company_members_4\":[\n",
    "        (\"NormalizedUserNameRaw\", \"Username\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"FormattedNameRaw\", \"Name\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"FamilyName\", \"Family name\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"GivenName\", \"Given name\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Email address\", np.nan, np.nan),\n",
    "        (\"Gender\", \"Gender\", \"\", \"\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"UnformattedNumber\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Phone numbers\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Name\", np.nan, np.nan)\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/user_profiles/company_members_5\":[\n",
    "        (\"NormalizedUserNameRaw\", \"Username\", np.nan, np.nan),\n",
    "        (\"FormattedNameRaw\", \"Name\", np.nan, np.nan),\n",
    "        (\"FamilyName\", \"Family name\", np.nan, np.nan),\n",
    "        (\"GivenName\", \"Given name\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Email address\"),\n",
    "        (\"Gender\", \"Gender\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"UnformattedNumber\"),\n",
    "        (\"\", \"\", \"\", \"Phone numbers\")\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/user_profiles/company_members_6\":[\n",
    "        (\"NormalizedUserNameRaw\", \"Username\", np.nan, np.nan),\n",
    "        (\"FormattedNameRaw\", \"Name\", np.nan, np.nan),\n",
    "        (\"FamilyName\", \"Family name\", np.nan, np.nan),\n",
    "        (\"GivenName\", \"Given name\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Email address\"),\n",
    "        (\"Gender\", \"Gender\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"UnformattedNumber\"),\n",
    "        (\"\", \"\", \"\", \"Phone numbers\")\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/user_profiles/company_members_7\":[\n",
    "        (\"NormalizedUserNameRaw\", \"Username\", np.nan, np.nan),\n",
    "        (\"FormattedNameRaw\", \"Name\", np.nan, np.nan),\n",
    "        (\"FamilyName\", \"Family name\", np.nan, np.nan),\n",
    "        (\"GivenName\", \"Given name\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Email address\"),\n",
    "        (\"Gender\", \"Gender\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"UnformattedNumber\"),\n",
    "        (\"\", \"\", \"\", \"Phone numbers\")\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/user_profiles/company_members_8\":[\n",
    "        (\"NormalizedUserNameRaw\", \"Username\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"FormattedNameRaw\", \"Name\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"FamilyName\", \"Family name\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"GivenName\", \"Given name\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Email address\", np.nan, np.nan),\n",
    "        (\"Gender\", \"Gender\", \"\", \"\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"UnformattedNumber\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Phone numbers\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Name\", np.nan, np.nan)\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/user_profiles/company_members_9\":[\n",
    "        (\"NormalizedUserNameRaw\", \"Username\", np.nan, np.nan),\n",
    "        (\"FormattedNameRaw\", \"Name\", np.nan, np.nan),\n",
    "        (\"FamilyName\", \"Family name\", np.nan, np.nan),\n",
    "        (\"GivenName\", \"Given name\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Email address\"),\n",
    "        (\"Gender\", \"Gender\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"UnformattedNumber\"),\n",
    "        (\"\", \"\", \"\", \"Phone numbers\")\n",
    "    ],\n",
    "    \"Workplace Data Company Information_pkl_converted/user_profiles/company_members_10\":[\n",
    "        (\"NormalizedUserNameRaw\", \"Username\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"FormattedNameRaw\", \"Name\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"FamilyName\", \"Family name\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"GivenName\", \"Given name\", np.nan, np.nan, np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Email address\", np.nan, np.nan),\n",
    "        (\"Gender\", \"Gender\", \"\", \"\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"UnformattedNumber\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Phone numbers\", np.nan, np.nan),\n",
    "        (\"\", \"\", \"\", \"Name\", np.nan, np.nan)\n",
    "    ],\n",
    "    \"likes_and_reactions_\": [\n",
    "        ('', '', '', 'Name'),\n",
    "        (\"Name\", \"Name\", np.nan, np.nan)\n",
    "    ],\n",
    "    \"your_messages_\": [\n",
    "        ('', '', '', 'Name', np.nan),\n",
    "        ('', '', '', 'IP address', np.nan)\n",
    "    ],\n",
    "    \"your_videos_\": [\n",
    "        ('', '', '', 'Upload IP')\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
