{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def search_strings_in_json(directory, strings_to_find):\n",
    "    matches = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        content = f.read()\n",
    "                        for string in strings_to_find:\n",
    "                            if string in content.lower():\n",
    "                                matches.append((filepath, string))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {filepath}: {e}\")\n",
    "    return matches\n",
    "\n",
    "# strings_to_find = ['2307148732639320', '2090275401330376', '2090275404663709', '100056956201608']\n",
    "strings_to_find=['8073719999360913']\n",
    "\n",
    "#company info and company group:61565211886319 --> people_sets_1 & child_groups, 1910535509304367\n",
    "\n",
    "#7775445322516960\n",
    "#565159963841935\n",
    "#1910351919322726\n",
    "#100040716736805\n",
    "# directory = '/Users/dan/Documents/local_code/test/data/woolies/workplace/organization'\n",
    "# directory = '/Users/dan/Documents/local_code/test/data/woolies/workplace/groups'\n",
    "# directory = 'Workplace Data Company Information/groups'\n",
    "# directory = 'Workplace Data Company Information/organization'\n",
    "# directory = 'Workplace Data Company Information/user_profiles'\n",
    "directory = 'Workplace Data Company Information/groups'\n",
    "matches = search_strings_in_json(directory, strings_to_find)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for match in matches:\n",
    "    count+=1\n",
    "    print(f\"Found '{match[1]}' in file: {match[0]}\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON to pkl converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "def read_json_files(root_dir):\n",
    "    data = {}\n",
    "    if root_dir.endswith('.json'):\n",
    "        temp_key_name = root_dir.split('/')[-1].split('.')[0]\n",
    "        with open(root_dir, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)\n",
    "                data[temp_key_name] = json_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error reading {root_dir}: {e}\")\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.json'):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        temp_key_name = root_dir + '/' + file\n",
    "                        try:\n",
    "                            json_data = json.load(f)\n",
    "                            data[temp_key_name] = json_data\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error reading {filepath}: {e}\")\n",
    "    return data\n",
    "\n",
    "def extract_company_group(data_dict):\n",
    "    data_dict = list(data_dict.values())[0]\n",
    "    records = []\n",
    "    record_template = {(key, key): value for key, value in data_dict.items() if key != 'label_values'}\n",
    "    \n",
    "    \n",
    "\n",
    "    from copy import deepcopy\n",
    "\n",
    "    for item in data_dict['label_values']:\n",
    "        # print(item)\n",
    "        if not \"dict\" in item:\n",
    "            record_template[(item['ent_field_name'], item['label'])] = item.get('value', np.nan)\n",
    "\n",
    "    for item in data_dict['label_values']:\n",
    "        if \"dict\" not in item: continue\n",
    "\n",
    "        record = deepcopy(record_template)\n",
    "        # print(item)\n",
    "        record[(\"title\", \"title\")] = item['title']\n",
    "        record_copy = deepcopy(record)\n",
    "        \n",
    "        entry_list = item['dict']\n",
    "\n",
    "        for entry in entry_list:\n",
    "            for s_entry in entry['dict']:\n",
    "                # print(s_entry)\n",
    "\n",
    "                record_copy[(s_entry['ent_field_name'], s_entry.get('label', s_entry['ent_field_name']))] = s_entry['value']\n",
    "\n",
    "            records.append(deepcopy(record_copy))\n",
    "\n",
    "    column_index = pd.MultiIndex.from_tuples(list(records[0].keys()))\n",
    "    df = pd.DataFrame.from_records(records, columns=column_index)\n",
    "\n",
    "    # print(records)\n",
    "\n",
    "    return df\n",
    "\n",
    "# def extract_data(data, file_name = \"\"):\n",
    "#     # if file_name.startswith('company_group'):\n",
    "#     #     return extract_company_group(data)\n",
    "    \n",
    "#     records = []\n",
    "#     columns = set()\n",
    "\n",
    "#     def check_exists_append(record, key, value, columns):\n",
    "#         if record.get(key, False):\n",
    "#             if isinstance(record[key], list):\n",
    "#                 record[key].append(value)\n",
    "#             else:\n",
    "#                 record[key] = [record[key]] + [value]\n",
    "#         else:\n",
    "#             record[key] = value\n",
    "#             columns.add(key)\n",
    "\n",
    "#     def process_data(item, parent_record, parent_labels):\n",
    "#         new_record = 0\n",
    "#         # Copy the parent record to avoid mutation\n",
    "#         record = parent_record.copy()\n",
    "#         labels = parent_labels.copy()\n",
    "\n",
    "#         # Make a new entry if we're at the top level\n",
    "#         for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "#             if key in item:\n",
    "#                 new_record = 1\n",
    "\n",
    "#         if new_record == 1:\n",
    "#             record = {}.copy()\n",
    "\n",
    "#         # Extract basic fields\n",
    "#         for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "#             if key in item:\n",
    "#                 record[key] = item[key]\n",
    "#                 columns.add((key, \"\"))\n",
    "\n",
    "#         # Process 'label_values'\n",
    "#         label_values = item.get('label_values', [])\n",
    "#         if label_values:\n",
    "#             for lv in label_values:\n",
    "#                 ent_field_name = lv.get('ent_field_name', '')\n",
    "#                 label = lv.get('label', '')\n",
    "#                 key_name = [ent_field_name, label]\n",
    "\n",
    "#                 if 'value' in lv or 'timestamp_value' in lv:\n",
    "#                     value = lv.get('value') or lv.get('timestamp_value')\n",
    "#                     if labels:\n",
    "#                         check_exists_append(record, tuple(labels), value, columns)\n",
    "\n",
    "#                         # records.append(record.copy())\n",
    "#                     else:\n",
    "#                         check_exists_append(record, tuple(key_name), value, columns)\n",
    "#                 elif 'vec' in lv:\n",
    "#                     # Handle 'dict' and 'vec' recursively\n",
    "#                     nested_items = lv.get('vec')\n",
    "#                     if nested_items:\n",
    "#                         for nested_item in nested_items:\n",
    "#                             process_data({'label_values': [nested_item]}, parent_record, key_name )\n",
    "#                 elif 'dict' in lv:\n",
    "#                     # print(lv)\n",
    "#                     for entry in lv.get('dict', {}):\n",
    "#                         value = entry.get('value') or entry.get('timestamp_value')\n",
    "#                         temp_label = entry.get('label', '')\n",
    "#                         temp_key_name = (ent_field_name, temp_label)\n",
    "#                         if labels:\n",
    "#                             check_exists_append(record, tuple(labels + [temp_label]), value, columns)\n",
    "#                         else:\n",
    "#                             check_exists_append(record, tuple(key_name + [temp_label]), value, columns)\n",
    "#         else:\n",
    "#             # Handle case for generic JSON file\n",
    "#             for key, value in item.items():\n",
    "#                 check_exists_append(record, tuple([key]), value, columns)\n",
    "#         records.append(record.copy())\n",
    "\n",
    "#     # Iterate through a list\n",
    "#     for data_value in data.values():\n",
    "#         if isinstance(data_value, list):\n",
    "#             for item in data_value:\n",
    "#                 process_data(item, {}, [])\n",
    "#         else:\n",
    "#             process_data(data_value, {}, [])\n",
    "\n",
    "\n",
    "#     def create_multiindex_df(data):\n",
    "#         # Function to flatten dictionary and preserve both tuple elements\n",
    "#         def flatten_dict(d):\n",
    "#             flattened = {}\n",
    "#             for k, v in d.items():\n",
    "#                 if isinstance(k, tuple):\n",
    "#                     # Store both parts of the tuple\n",
    "#                     flattened[k] = v\n",
    "#                 else:\n",
    "#                     # For non-tuple keys, create a tuple with same value\n",
    "#                     flattened[(k, k)] = v\n",
    "#             return flattened\n",
    "        \n",
    "#         # Flatten all dictionaries\n",
    "#         flattened_data = [flatten_dict(d) for d in data]\n",
    "        \n",
    "#         # Create initial DataFrame\n",
    "#         df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "#         # Get all unique column tuples\n",
    "#         columns = df.columns.tolist()\n",
    "        \n",
    "\n",
    "#         # Create MultiIndex columns\n",
    "#         multi_index = pd.MultiIndex.from_tuples(columns)\n",
    "        \n",
    "#         # Create final DataFrame with MultiIndex\n",
    "#         final_df = pd.DataFrame(df.values, columns=multi_index)\n",
    "        \n",
    "#         return final_df\n",
    "#     return create_multiindex_df(records)\n",
    "\n",
    "def extract_data_old(data, file_name = \"\"):\n",
    "    records = []\n",
    "    columns = set()\n",
    "\n",
    "    def check_exists_append(record, key, value, columns):\n",
    "        if record.get(key, False):\n",
    "            if isinstance(record[key], list):\n",
    "                record[key].append(value)\n",
    "            else:\n",
    "                record[key] = [record[key]] + [value]\n",
    "        else:\n",
    "            record[key] = value\n",
    "            columns.add(key)\n",
    "\n",
    "    def process_vec_items(vec_items, record, key_name, parent_labels):\n",
    "        \"\"\"Helper function to process vector items recursively\"\"\"\n",
    "        if vec_items:\n",
    "            for nested_item in vec_items:\n",
    "                process_data({'label_values': [nested_item]}, record, key_name)\n",
    "\n",
    "    def process_dict_items(dict_items, record, ent_field_name, labels):\n",
    "        \"\"\"Helper function to process dictionary items recursively\"\"\"\n",
    "        for entry in dict_items:\n",
    "            value = entry.get('value') or entry.get('timestamp_value')\n",
    "            temp_label = entry.get('label', '')\n",
    "            \n",
    "            # Handle nested vectors within dictionary\n",
    "            if 'vec' in entry:\n",
    "                nested_key_name = labels + [temp_label] if temp_label else labels + [ent_field_name]\n",
    "                process_vec_items(entry['vec'], record, nested_key_name, labels)\n",
    "            # Handle nested dictionaries\n",
    "            elif 'dict' in entry:\n",
    "                nested_labels = labels + [temp_label] if temp_label else labels + [ent_field_name]\n",
    "                process_dict_items(entry['dict'], record, ent_field_name, nested_labels)\n",
    "            # Handle leaf values\n",
    "            else:\n",
    "                if labels:\n",
    "                    if temp_label:\n",
    "                        check_exists_append(record, tuple(labels + [temp_label]), value, columns)\n",
    "                    else:\n",
    "                        check_exists_append(record, tuple(labels + [ent_field_name]), value, columns)\n",
    "                else:\n",
    "                    check_exists_append(record, tuple([ent_field_name, temp_label]), value, columns)\n",
    "\n",
    "    def process_data(item, parent_record, parent_labels):\n",
    "        new_record = 0\n",
    "        record = parent_record.copy()\n",
    "        labels = parent_labels.copy()\n",
    "\n",
    "        # Make a new entry if we're at the top level\n",
    "        for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "            if key in item:\n",
    "                new_record = 1\n",
    "\n",
    "        if new_record == 1:\n",
    "            record = {}.copy()\n",
    "\n",
    "        # Extract basic fields\n",
    "        for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "            if key in item:\n",
    "                record[key] = item[key]\n",
    "                columns.add((key, \"\"))\n",
    "\n",
    "        # Process 'label_values'\n",
    "        label_values = item.get('label_values', [])\n",
    "        if label_values:\n",
    "            for lv in label_values:\n",
    "                ent_field_name = lv.get('ent_field_name', '')\n",
    "                label = lv.get('label', '')\n",
    "                key_name = [ent_field_name, label]\n",
    "\n",
    "                if 'value' in lv or 'timestamp_value' in lv:\n",
    "                    value = lv.get('value') or lv.get('timestamp_value')\n",
    "                    if labels:\n",
    "                        check_exists_append(record, tuple(labels), value, columns)\n",
    "                    else:\n",
    "                        check_exists_append(record, tuple(key_name), value, columns)\n",
    "                elif 'vec' in lv:\n",
    "                    # Handle vector items recursively\n",
    "                    process_vec_items(lv['vec'], record, key_name, labels)\n",
    "                elif 'dict' in lv:\n",
    "                    # Handle dictionary items recursively\n",
    "                    process_dict_items(lv['dict'], record, ent_field_name, \n",
    "                                    labels if labels else key_name)\n",
    "\n",
    "        else:\n",
    "            # Handle case for generic JSON file\n",
    "            for key, value in item.items():\n",
    "                check_exists_append(record, tuple([key]), value, columns)\n",
    "\n",
    "        records.append(record.copy())\n",
    "\n",
    "    # Iterate through a list\n",
    "    for data_value in data.values():\n",
    "        if isinstance(data_value, list):\n",
    "            for item in data_value:\n",
    "                process_data(item, {}, [])\n",
    "        else:\n",
    "            process_data(data_value, {}, [])\n",
    "\n",
    "    def create_multiindex_df(data):\n",
    "        # Function to flatten dictionary and preserve both tuple elements\n",
    "        def flatten_dict(d):\n",
    "            flattened = {}\n",
    "            for k, v in d.items():\n",
    "                if isinstance(k, tuple):\n",
    "                    # Store both parts of the tuple\n",
    "                    flattened[k] = v\n",
    "                else:\n",
    "                    # For non-tuple keys, create a tuple with same value\n",
    "                    flattened[(k, k)] = v\n",
    "            return flattened\n",
    "        \n",
    "        # Flatten all dictionaries\n",
    "        flattened_data = [flatten_dict(d) for d in data]\n",
    "        \n",
    "        # Create initial DataFrame\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "        # Get all unique column tuples\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        # Create MultiIndex columns\n",
    "        multi_index = pd.MultiIndex.from_tuples(columns)\n",
    "        \n",
    "        # Create final DataFrame with MultiIndex\n",
    "        final_df = pd.DataFrame(df.values, columns=multi_index)\n",
    "\n",
    "        def remove_duplicates_with_lists(df):\n",
    "            # Convert lists to strings for comparison\n",
    "            df_comparable = df.copy()\n",
    "            \n",
    "            # Convert empty lists to a string representation\n",
    "            df_comparable = df_comparable.applymap(lambda x: str(x) if isinstance(x, list) else x)\n",
    "            \n",
    "            # Drop duplicates based on all columns except the first one (assuming first is index)\n",
    "            # Keep first occurrence and preserve order\n",
    "            unique_indices = df_comparable.iloc[:, 1:].drop_duplicates().index\n",
    "            \n",
    "            # Return original dataframe with duplicates removed\n",
    "            return df.iloc[unique_indices]\n",
    "        \n",
    "        final_df_no_duplicates = remove_duplicates_with_lists(final_df)\n",
    "        \n",
    "        return final_df_no_duplicates\n",
    "    \n",
    "    return create_multiindex_df(records)\n",
    "\n",
    "# def extract_data(data, file_name = \"\"):\n",
    "#     records = []\n",
    "#     columns = set()\n",
    "\n",
    "#     def check_exists_append(record, key, value, columns, force_new=False):\n",
    "#         \"\"\"\n",
    "#         Append function that combines values into lists unless force_new is True\n",
    "#         \"\"\"\n",
    "#         if force_new:\n",
    "#             record[key] = value\n",
    "#             columns.add(key)\n",
    "#         else:\n",
    "#             if record.get(key, False):\n",
    "#                 if isinstance(record[key], list):\n",
    "#                     record[key].append(value)\n",
    "#                 else:\n",
    "#                     record[key] = [record[key]] + [value]\n",
    "#             else:\n",
    "#                 record[key] = value\n",
    "#                 columns.add(key)\n",
    "\n",
    "#     def process_vec_items(vec_items, base_record, key_name, parent_labels):\n",
    "#         \"\"\"Helper function to process vector items recursively\"\"\"\n",
    "#         if vec_items:\n",
    "#             for nested_item in vec_items:\n",
    "#                 process_data({'label_values': [nested_item]}, base_record.copy(), key_name)\n",
    "\n",
    "#     def has_nested_dict(dict_items):\n",
    "#         \"\"\"Check if there are nested dictionaries in the dict_items\"\"\"\n",
    "#         for entry in dict_items:\n",
    "#             if 'dict' in entry:\n",
    "#                 return True\n",
    "#         return False\n",
    "\n",
    "#     def process_dict_items(dict_items, base_record, ent_field_name, labels):\n",
    "#         \"\"\"Helper function to process dictionary items recursively\"\"\"\n",
    "#         has_nested = has_nested_dict(dict_items)\n",
    "#         current_record = base_record.copy()\n",
    "        \n",
    "#         for entry in dict_items:\n",
    "#             value = entry.get('value') or entry.get('timestamp_value')\n",
    "#             temp_label = entry.get('label', '')\n",
    "#             nested_ent_field_name = entry.get('ent_field_name', '')\n",
    "            \n",
    "#             # If there's an ent_field_name in the entry, use it\n",
    "#             current_field_name = nested_ent_field_name if nested_ent_field_name else ent_field_name\n",
    "            \n",
    "#             # Handle nested vectors within dictionary\n",
    "#             if 'vec' in entry:\n",
    "#                 nested_key_name = labels + [temp_label or current_field_name]\n",
    "#                 process_vec_items(entry['vec'], current_record, nested_key_name, labels)\n",
    "#             # Handle nested dictionaries\n",
    "#             elif 'dict' in entry:\n",
    "#                 nested_labels = labels + [temp_label or current_field_name]\n",
    "#                 process_dict_items(entry['dict'], current_record, current_field_name, nested_labels)\n",
    "#             # Handle leaf values\n",
    "#             else:\n",
    "#                 if labels:\n",
    "#                     key = tuple(labels + [temp_label or current_field_name])\n",
    "#                 else:\n",
    "#                     key = tuple([current_field_name, temp_label]) if temp_label else tuple([current_field_name, current_field_name])\n",
    "                \n",
    "#                 check_exists_append(current_record, key, value, columns, force_new=has_nested)\n",
    "        \n",
    "#         # Only append if record has data\n",
    "#         if len(current_record) > 0:\n",
    "#             records.append(current_record)\n",
    "\n",
    "#     def process_data(item, parent_record, parent_labels):\n",
    "#         record = parent_record.copy()\n",
    "#         labels = parent_labels.copy()\n",
    "\n",
    "#         # Extract basic fields\n",
    "#         for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "#             if key in item:\n",
    "#                 record[key] = item[key]\n",
    "#                 columns.add((key, \"\"))\n",
    "\n",
    "#         # Process 'label_values'\n",
    "#         label_values = item.get('label_values', [])\n",
    "#         if label_values:\n",
    "#             for lv in label_values:\n",
    "#                 ent_field_name = lv.get('ent_field_name', '')\n",
    "#                 label = lv.get('label', '')\n",
    "#                 key_name = [ent_field_name, label] if label else [ent_field_name, ent_field_name]\n",
    "\n",
    "#                 if 'value' in lv or 'timestamp_value' in lv:\n",
    "#                     value = lv.get('value') or lv.get('timestamp_value')\n",
    "#                     if labels:\n",
    "#                         check_exists_append(record, tuple(labels), value, columns)\n",
    "#                     else:\n",
    "#                         check_exists_append(record, tuple(key_name), value, columns)\n",
    "#                 elif 'vec' in lv:\n",
    "#                     # Handle vector items recursively\n",
    "#                     process_vec_items(lv['vec'], record, key_name, labels)\n",
    "#                 elif 'dict' in lv:\n",
    "#                     # Handle dictionary items recursively\n",
    "#                     process_dict_items(lv['dict'], record, ent_field_name, \n",
    "#                                     labels if labels else key_name)\n",
    "\n",
    "#         else:\n",
    "#             # Handle case for generic JSON file\n",
    "#             for key, value in item.items():\n",
    "#                 check_exists_append(record, tuple([key]), value, columns)\n",
    "\n",
    "#         # Only append if record has data and is not already included\n",
    "#         if len(record) > 0 and record not in records:\n",
    "#             records.append(record.copy())\n",
    "\n",
    "#     # Iterate through data\n",
    "#     if isinstance(data, dict):\n",
    "#         for data_value in data.values():\n",
    "#             if isinstance(data_value, list):\n",
    "#                 for item in data_value:\n",
    "#                     process_data(item, {}, [])\n",
    "#             else:\n",
    "#                 process_data(data_value, {}, [])\n",
    "\n",
    "#     def create_multiindex_df(data):\n",
    "#         if not data:\n",
    "#             # Return empty DataFrame with proper structure\n",
    "#             return pd.DataFrame(columns=pd.MultiIndex.from_tuples([('empty', '')]))\n",
    "            \n",
    "#         # Function to flatten dictionary and preserve both tuple elements\n",
    "#         def flatten_dict(d):\n",
    "#             flattened = {}\n",
    "#             for k, v in d.items():\n",
    "#                 if isinstance(k, tuple):\n",
    "#                     # Store both parts of the tuple\n",
    "#                     flattened[k] = v\n",
    "#                 else:\n",
    "#                     # For non-tuple keys, create a tuple with same value\n",
    "#                     flattened[(k, k)] = v\n",
    "#             return flattened\n",
    "        \n",
    "#         # Flatten all dictionaries\n",
    "#         flattened_data = [flatten_dict(d) for d in data]\n",
    "        \n",
    "#         # Create initial DataFrame\n",
    "#         df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "#         if df.empty:\n",
    "#             return df\n",
    "            \n",
    "#         # Get all unique column tuples\n",
    "#         columns = df.columns.tolist()\n",
    "        \n",
    "#         # Create MultiIndex columns\n",
    "#         multi_index = pd.MultiIndex.from_tuples(columns)\n",
    "        \n",
    "#         # Create final DataFrame with MultiIndex\n",
    "#         final_df = pd.DataFrame(df.values, columns=multi_index)\n",
    "        \n",
    "#         return final_df\n",
    "    \n",
    "#     return create_multiindex_df(records)\n",
    "\n",
    "def is_redundant_record(prev_record, current_record):\n",
    "    \"\"\"Check if the current record is redundant with the previous record\"\"\"\n",
    "    if not prev_record or not current_record:\n",
    "        return False\n",
    "    for key, value in current_record.items():\n",
    "        if not value:\n",
    "            continue\n",
    "        if key not in prev_record or prev_record[key] != value:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def extract_data(data, file_name = \"\"):\n",
    "    records = []\n",
    "    columns = set()\n",
    "\n",
    "    def check_exists_append(record, key, value, columns, force_new=False, is_title=False):\n",
    "        \"\"\"\n",
    "        Append function that combines values into lists unless force_new is True\n",
    "        For titles, always overwrite instead of appending\n",
    "        \"\"\"\n",
    "        if is_title:\n",
    "            record[key] = value  # Simply overwrite for titles\n",
    "            columns.add(key)\n",
    "        elif force_new:\n",
    "            record[key] = value\n",
    "            columns.add(key)\n",
    "        else:\n",
    "            if record.get(key, False):\n",
    "                if isinstance(record[key], list):\n",
    "                    record[key].append(value)\n",
    "                else:\n",
    "                    record[key] = [record[key]] + [value]\n",
    "            else:\n",
    "                record[key] = value\n",
    "                columns.add(key)\n",
    "\n",
    "    def process_vec_items(vec_items, base_record, key_name, parent_labels):\n",
    "        \"\"\"Helper function to process vector items recursively\"\"\"\n",
    "        if vec_items:\n",
    "            for nested_item in vec_items:\n",
    "                process_data({'label_values': [nested_item]}, base_record.copy(), key_name)\n",
    "\n",
    "    def process_dict_items(dict_items, base_record, ent_field_name, labels, parent_title=None, depth=0):\n",
    "        \"\"\"Helper function to process dictionary items recursively\"\"\"\n",
    "        # if not has_nested:\n",
    "        #     has_nested = has_nested_dict(dict_items)\n",
    "        current_record = base_record if depth == 0 else base_record.copy()\n",
    "        \n",
    "        # Add parent title to record if it exists\n",
    "        if parent_title:\n",
    "            title_key = tuple([ent_field_name, 'title'])\n",
    "            check_exists_append(current_record, title_key, parent_title, columns, is_title=True)\n",
    "        \n",
    "        for entry in dict_items:\n",
    "            value = entry.get('value') or entry.get('timestamp_value')\n",
    "            temp_label = entry.get('label', '')\n",
    "            nested_ent_field_name = entry.get('ent_field_name', '')\n",
    "            \n",
    "            # If there's an ent_field_name in the entry, use it\n",
    "            current_field_name = nested_ent_field_name if nested_ent_field_name else ent_field_name\n",
    "            \n",
    "            # Handle nested vectors within dictionary\n",
    "            if 'vec' in entry:\n",
    "                nested_key_name = labels + [temp_label or current_field_name]\n",
    "                process_vec_items(entry['vec'], current_record, nested_key_name, labels)\n",
    "            # Handle nested dictionaries\n",
    "            elif 'dict' in entry:\n",
    "                nested_labels = labels + [temp_label or current_field_name]\n",
    "                process_dict_items(entry['dict'], current_record, current_field_name, \n",
    "                                nested_labels, parent_title, depth=depth+1)\n",
    "            # Handle leaf values\n",
    "            else:\n",
    "                if labels:\n",
    "                    key = tuple(labels + [temp_label or current_field_name])\n",
    "                else:\n",
    "                    key = tuple([current_field_name, temp_label]) if temp_label else tuple([current_field_name, current_field_name])\n",
    "                \n",
    "                check_exists_append(current_record, key, value, columns, force_new=depth > 0)\n",
    "        \n",
    "        # Only append if record has data\n",
    "        if depth > 0 and current_record not in records and dict_items:\n",
    "            records.append(current_record)\n",
    "\n",
    "    def process_data(item, parent_record, parent_labels):\n",
    "        record = parent_record.copy()\n",
    "        labels = parent_labels.copy()\n",
    "\n",
    "        # Extract basic fields\n",
    "        for key in ['timestamp', 'media', 'fbid', 'ent_name']:\n",
    "            if key in item:\n",
    "                record[key] = item[key]\n",
    "                columns.add((key, \"\"))\n",
    "\n",
    "        # Process 'label_values'\n",
    "        label_values = item.get('label_values', [])\n",
    "        if label_values:\n",
    "            for lv in label_values:\n",
    "                ent_field_name = lv.get('ent_field_name', '')\n",
    "                label = lv.get('label', '')\n",
    "                key_name = [ent_field_name, label] if label else [ent_field_name, ent_field_name]\n",
    "                \n",
    "                if 'value' in lv or 'timestamp_value' in lv:\n",
    "                    value = lv.get('value') or lv.get('timestamp_value')\n",
    "                    if labels:\n",
    "                        check_exists_append(record, tuple(labels), value, columns)\n",
    "                    else:\n",
    "                        check_exists_append(record, tuple(key_name), value, columns)\n",
    "                elif 'vec' in lv:\n",
    "                    # Handle vector items recursively\n",
    "                    process_vec_items(lv['vec'], record, key_name, labels)\n",
    "                elif 'dict' in lv:\n",
    "                    # Extract title before processing dict\n",
    "                    dict_title = lv.get('title')\n",
    "                    # Handle dictionary items recursively with title\n",
    "                    process_dict_items(lv['dict'], record, ent_field_name, \n",
    "                                    labels if labels else key_name,\n",
    "                                    dict_title)\n",
    "                    # # If dict is empty but has title, create a record with just the title\n",
    "                    # if not lv['dict'] and dict_title:\n",
    "                    #     title_record = record.copy()\n",
    "                    #     title_key = tuple([ent_field_name, 'title'])\n",
    "                    #     check_exists_append(title_record, title_key, dict_title, columns, is_title=True)\n",
    "                    #     records.append(title_record)\n",
    "                    # continue  # Skip the final record append as it's handled in process_dict_items\n",
    "\n",
    "        else:\n",
    "            # Handle case for generic JSON file\n",
    "            for key, value in item.items():\n",
    "                check_exists_append(record, tuple([key]), value, columns)\n",
    "\n",
    "        if len(records) > 0 and is_redundant_record(records[-1], record):\n",
    "            return\n",
    "\n",
    "        # Only append if record has data and is not already included\n",
    "        if len(record) > 0:\n",
    "            records.append(record.copy())\n",
    "\n",
    "    # Iterate through data\n",
    "    if isinstance(data, dict):\n",
    "        for data_value in data.values():\n",
    "            if isinstance(data_value, list):\n",
    "                for item in data_value:\n",
    "                    process_data(item, {}, [])\n",
    "            else:\n",
    "                process_data(data_value, {}, [])\n",
    "\n",
    "    def create_multiindex_df(data):\n",
    "        if not data:\n",
    "            # Return empty DataFrame with proper structure\n",
    "            return pd.DataFrame(columns=pd.MultiIndex.from_tuples([('empty', '')]))\n",
    "            \n",
    "        # Function to flatten dictionary and preserve both tuple elements\n",
    "        def flatten_dict(d):\n",
    "            flattened = {}\n",
    "            for k, v in d.items():\n",
    "                if isinstance(k, tuple):\n",
    "                    # Store both parts of the tuple\n",
    "                    flattened[k] = v\n",
    "                else:\n",
    "                    # For non-tuple keys, create a tuple with same value\n",
    "                    flattened[(k, k)] = v\n",
    "            return flattened\n",
    "        \n",
    "        # Flatten all dictionaries\n",
    "        flattened_data = [flatten_dict(d) for d in data]\n",
    "        \n",
    "        # Create initial DataFrame\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        \n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        # Get all unique column tuples\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        # Create MultiIndex columns\n",
    "        multi_index = pd.MultiIndex.from_tuples(columns)\n",
    "        \n",
    "        # Create final DataFrame with MultiIndex\n",
    "        final_df = pd.DataFrame(df.values, columns=multi_index)\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    return create_multiindex_df(records)\n",
    "\n",
    "\n",
    "def convert_to_pkl(root_dir):\n",
    "    assert not root_dir.endswith('.json'), f\"ERROR: Path must be a directory not a file {root_dir}\"\n",
    "    root_temp = \"\"\n",
    "    folder_loc = -1\n",
    "    new_name = \"default_csv\"\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        # On first iteration we're setting up temp variables\n",
    "        if not root_temp:\n",
    "            root_temp = root.split(\"/\")\n",
    "            folder_loc = len(root_temp) - 1\n",
    "            new_name = root_temp[folder_loc] + \"_pkl_converted\"\n",
    "        \n",
    "        # For each file in each directory\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                # Create new output path\n",
    "                output_root = root.split(\"/\")\n",
    "                output_root[folder_loc] = new_name\n",
    "                output_root = \"/\".join(output_root)\n",
    "                \n",
    "                # Process the file\n",
    "                filepath = os.path.join(root, file)\n",
    "                print(f\"Processing {filepath}\")\n",
    "                \n",
    "                temp_data = read_json_files(filepath)\n",
    "                temp_records = extract_data(temp_data, file)\n",
    "                \n",
    "                # Create output directory if it doesn't exist\n",
    "                os.makedirs(output_root, exist_ok=True)\n",
    "                \n",
    "                # Save each DataFrame in the dictionary\n",
    "                output_filename = file.replace('.json', '.pkl')\n",
    "                output_csv_filename = file.replace('.json', '.csv')\n",
    "                output_path = os.path.join(output_root, output_filename)\n",
    "                output_csv_path = os.path.join(output_root, output_csv_filename)\n",
    "                temp_records.to_pickle(output_path, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                temp_records.to_csv(output_csv_path)\n",
    "                print(f\"Saved {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_data = read_json_files('Workplace Data Company Information/organization/company_group_1.json')\n",
    "df = extract_data(temp_data)\n",
    "df.to_csv('company_group_1.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def extract_company_info(data_dict):\n",
    "#     \"\"\"\n",
    "#     Extracts company information from a JSON structure.\n",
    "#     Long description fields are not used as keys.\n",
    "#     \"\"\"\n",
    "#     records = []\n",
    "#     # Build a template from the top-level keys (except 'label_values')\n",
    "#     record_template = {(key, key): value for key, value in data_dict.items() if key != 'label_values'}\n",
    "    \n",
    "#     # For non-nested (simple) entries, update the template.\n",
    "#     for item in data_dict.get('label_values', []):\n",
    "#         if \"dict\" not in item and \"vec\" not in item:\n",
    "#             record_template[(item['ent_field_name'], item.get('label', np.nan))] = item.get('value', np.nan)\n",
    "    \n",
    "#     # Handle nested entries (records with a \"dict\" key)\n",
    "#     for item in data_dict.get('label_values', []):\n",
    "#         if \"dict\" not in item or \"vec\" not in item:\n",
    "#             continue\n",
    "#         base_record = deepcopy(record_template)\n",
    "#         # Check if the nested entries are grouped (e.g., group installs) by looking for a nested \"dict\" inside.\n",
    "#         first_nested = item['dict'][0] if item['dict'] else None\n",
    "#         if first_nested and isinstance(first_nested, dict) and 'dict' in first_nested:\n",
    "#             # This is a group of nested records\n",
    "#             for group_item in item['dict']:\n",
    "#                 record = deepcopy(base_record)\n",
    "#                 if 'title' in group_item:\n",
    "#                     record[(\"title\", \"title\")] = group_item['title']\n",
    "#                 for sub_entry in group_item.get('dict', []):\n",
    "#                     record[(sub_entry['ent_field_name'], sub_entry.get('label', sub_entry['ent_field_name']))] = sub_entry.get('value', np.nan)\n",
    "#                 records.append(record)\n",
    "#         else:\n",
    "#             # Otherwise, the nested dict is at one level.\n",
    "#             record = deepcopy(base_record)\n",
    "#             for sub_entry in item['dict']:\n",
    "#                 record[(sub_entry['ent_field_name'], sub_entry.get('label', sub_entry['ent_field_name']))] = sub_entry.get('value', np.nan)\n",
    "#             records.append(record)\n",
    "    \n",
    "#     if records:\n",
    "#         column_index = pd.MultiIndex.from_tuples(list(records[0].keys()))\n",
    "#         df = pd.DataFrame.from_records(records, columns=column_index)\n",
    "#     else:\n",
    "#         df = pd.DataFrame()\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# with open('Workplace Data Company Information/organization/company_info_1.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# df = extract_company_info(data)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>media</th>\n",
       "      <th>fbid</th>\n",
       "      <th>ent_name</th>\n",
       "      <th>ExternalID</th>\n",
       "      <th>CreationMethod</th>\n",
       "      <th>Members</th>\n",
       "      <th>NameRaw</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>UpdateTime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>media</th>\n",
       "      <th>fbid</th>\n",
       "      <th>ent_name</th>\n",
       "      <th>External ID</th>\n",
       "      <th>Creation Method</th>\n",
       "      <th>List of EntScimCompanyUser IDs which related to that people set</th>\n",
       "      <th>People set name</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Update time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>FBID</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1700774648</td>\n",
       "      <td>[]</td>\n",
       "      <td>2065784710446112</td>\n",
       "      <td>EntScimCompanyGroup</td>\n",
       "      <td>None</td>\n",
       "      <td>Admin</td>\n",
       "      <td>[100048806529329, 100029807447446, 10001166228...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1700774648</td>\n",
       "      <td>[]</td>\n",
       "      <td>2065784710446112</td>\n",
       "      <td>EntScimCompanyGroup</td>\n",
       "      <td>None</td>\n",
       "      <td>Admin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>knowledge#set#22HBwQXa9N</td>\n",
       "      <td>Knowledge audience</td>\n",
       "      <td>1700774650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1688517324</td>\n",
       "      <td>[]</td>\n",
       "      <td>1972752263082691</td>\n",
       "      <td>EntScimCompanyGroup</td>\n",
       "      <td>None</td>\n",
       "      <td>Admin</td>\n",
       "      <td>100020482295927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1688517324</td>\n",
       "      <td>[]</td>\n",
       "      <td>1972752263082691</td>\n",
       "      <td>EntScimCompanyGroup</td>\n",
       "      <td>None</td>\n",
       "      <td>Admin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EntWorkTeam#people#set#ZlwOr5CTbJ</td>\n",
       "      <td>Utility set for has people sets ent</td>\n",
       "      <td>1694617180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1686183965</td>\n",
       "      <td>[]</td>\n",
       "      <td>1954940858197165</td>\n",
       "      <td>EntScimCompanyGroup</td>\n",
       "      <td>None</td>\n",
       "      <td>Admin</td>\n",
       "      <td>100078697891602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>1556735630</td>\n",
       "      <td>[]</td>\n",
       "      <td>836451583379437</td>\n",
       "      <td>EntScimCompanyGroup</td>\n",
       "      <td>None</td>\n",
       "      <td>Admin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No IDs</td>\n",
       "      <td>Admin</td>\n",
       "      <td>1694617179.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>1552619167</td>\n",
       "      <td>[]</td>\n",
       "      <td>808344766190119</td>\n",
       "      <td>EntScimCompanyGroup</td>\n",
       "      <td>None</td>\n",
       "      <td>Admin</td>\n",
       "      <td>[100034003484642, 100021941614266, 10001386155...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1552619167</td>\n",
       "      <td>[]</td>\n",
       "      <td>808344766190119</td>\n",
       "      <td>EntScimCompanyGroup</td>\n",
       "      <td>None</td>\n",
       "      <td>Admin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHCH Team 15th</td>\n",
       "      <td>Admin</td>\n",
       "      <td>1694617179.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1543958767</td>\n",
       "      <td>[]</td>\n",
       "      <td>747690798922183</td>\n",
       "      <td>EntScimCompanyGroup</td>\n",
       "      <td>None</td>\n",
       "      <td>Admin</td>\n",
       "      <td>[100027369889758, 100013593523708, 10001350802...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1543958767</td>\n",
       "      <td>[]</td>\n",
       "      <td>747690798922183</td>\n",
       "      <td>EntScimCompanyGroup</td>\n",
       "      <td>None</td>\n",
       "      <td>Admin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Own Brand Prize Winners</td>\n",
       "      <td>Admin</td>\n",
       "      <td>1694617179.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp media              fbid             ent_name  ExternalID  \\\n",
       "      timestamp media              fbid             ent_name External ID   \n",
       "            NaN   NaN               NaN                  NaN         NaN   \n",
       "0    1700774648    []  2065784710446112  EntScimCompanyGroup        None   \n",
       "1    1700774648    []  2065784710446112  EntScimCompanyGroup        None   \n",
       "2    1688517324    []  1972752263082691  EntScimCompanyGroup        None   \n",
       "3    1688517324    []  1972752263082691  EntScimCompanyGroup        None   \n",
       "4    1686183965    []  1954940858197165  EntScimCompanyGroup        None   \n",
       "..          ...   ...               ...                  ...         ...   \n",
       "577  1556735630    []   836451583379437  EntScimCompanyGroup        None   \n",
       "578  1552619167    []   808344766190119  EntScimCompanyGroup        None   \n",
       "579  1552619167    []   808344766190119  EntScimCompanyGroup        None   \n",
       "580  1543958767    []   747690798922183  EntScimCompanyGroup        None   \n",
       "581  1543958767    []   747690798922183  EntScimCompanyGroup        None   \n",
       "\n",
       "     CreationMethod  \\\n",
       "    Creation Method   \n",
       "                NaN   \n",
       "0             Admin   \n",
       "1             Admin   \n",
       "2             Admin   \n",
       "3             Admin   \n",
       "4             Admin   \n",
       "..              ...   \n",
       "577           Admin   \n",
       "578           Admin   \n",
       "579           Admin   \n",
       "580           Admin   \n",
       "581           Admin   \n",
       "\n",
       "                                                            Members  \\\n",
       "    List of EntScimCompanyUser IDs which related to that people set   \n",
       "                                                               FBID   \n",
       "0    [100048806529329, 100029807447446, 10001166228...                \n",
       "1                                                  NaN                \n",
       "2                                      100020482295927                \n",
       "3                                                  NaN                \n",
       "4                                      100078697891602                \n",
       "..                                                 ...                \n",
       "577                                                NaN                \n",
       "578  [100034003484642, 100021941614266, 10001386155...                \n",
       "579                                                NaN                \n",
       "580  [100027369889758, 100013593523708, 10001350802...                \n",
       "581                                                NaN                \n",
       "\n",
       "                               NameRaw                              Purpose  \\\n",
       "                       People set name                              Purpose   \n",
       "                                   NaN                                  NaN   \n",
       "0                                  NaN                                  NaN   \n",
       "1             knowledge#set#22HBwQXa9N                   Knowledge audience   \n",
       "2                                  NaN                                  NaN   \n",
       "3    EntWorkTeam#people#set#ZlwOr5CTbJ  Utility set for has people sets ent   \n",
       "4                                  NaN                                  NaN   \n",
       "..                                 ...                                  ...   \n",
       "577                             No IDs                                Admin   \n",
       "578                                NaN                                  NaN   \n",
       "579                     CHCH Team 15th                                Admin   \n",
       "580                                NaN                                  NaN   \n",
       "581            Own Brand Prize Winners                                Admin   \n",
       "\n",
       "       UpdateTime  \n",
       "      Update time  \n",
       "              NaN  \n",
       "0             NaN  \n",
       "1    1700774650.0  \n",
       "2             NaN  \n",
       "3    1694617180.0  \n",
       "4             NaN  \n",
       "..            ...  \n",
       "577  1694617179.0  \n",
       "578           NaN  \n",
       "579  1694617179.0  \n",
       "580           NaN  \n",
       "581  1694617179.0  \n",
       "\n",
       "[582 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract and view single file\n",
    "temp_data = read_json_files('Workplace Data Company Information/groups/people_sets_1.json')\n",
    "temp_records = extract_data(temp_data)\n",
    "temp_records.to_csv('company_info_1.csv')\n",
    "temp_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert whole directory to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Workplace Data Company Information_pkl_converted/manifest.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/post_campaigns_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_contact_support_cases_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_6.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/promoted_post_notice_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_info_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_sso_settings_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/employee_safety_checkup_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/security_logs_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/email_domain_settings_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_7.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_level_installed_apps_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/community_events_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/workplace_team_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_reported_content_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/custom_badges_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_reported_content_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/work_invites_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/email_domains_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/crisis_information_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/security_logs_4.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_3.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/security_logs_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_4.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_info_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/bulk_tasks_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_8.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/profile_field_sets_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_9.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/promoted_post_notice_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_contact_support_cases_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/company_group_5.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/organization/security_logs_3.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_6.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_10.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_7.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_8.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_4.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_5.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_9.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/user_profiles/company_members_3.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_3.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/people_sets_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_11.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_8.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_31.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_27.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_4.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_5.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_26.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_30.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_10.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_9.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_37.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_21.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_2.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_5.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_17.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_40.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_41.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_16.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_4.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_3.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_20.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_8.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_36.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_19.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_23.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_35.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_42.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_15.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_7.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_39.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_38.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_6.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_14.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_43.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_34.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_22.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_18.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_44.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_13.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_29.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/multi_company_group_1.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_6.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_25.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_33.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_32.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_24.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_7.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_28.pkl\n",
      "Saved Workplace Data Company Information_pkl_converted/groups/child_groups_12.pkl\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'Workplace Data Company Information'\n",
    "convert_to_pkl(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "dict_list = [\n",
    "    {'name': 'Alice', 'age': 25},\n",
    "    {'name': 'Bob', 'number': [1, 2, 3]},\n",
    "    {'city': 'Paris', 'rating': 4.5}\n",
    "]\n",
    "\n",
    "# This will return False even though an identical dictionary exists in the list\n",
    "search_dict = {'name': 'Alice', 'age': 26}\n",
    "print(search_dict in dict_list)  # False\n",
    "\n",
    "# Even the same dictionary content at a different memory location won't match\n",
    "dict_list.append({'name': 'Alice', 'age': 25})\n",
    "print(search_dict in dict_list)  # Still False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workplace-converter-FMkCQOQs-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
